{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "from cross_validation import *\n",
    "from tools import *\n",
    "from implementations import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "seed = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "DATA_TEST_PATH = 'data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_correlation(tx):\n",
    "    corr = np.ones((tx.shape[1], tx.shape[1]))\n",
    "    for feature1 in range(0, tx.shape[1]):\n",
    "        for feature2 in range(0, tx.shape[1]):\n",
    "            corr[feature1, feature2] = np.corrcoef(tx[:, feature1], tx[:, feature2])[0, 1]\n",
    "            if (corr[feature1, feature2] >= 0.85 and feature1 != feature2):\n",
    "                print(\"Features {f1} and {f2} are highly correlated: {corr}\".format(f1 =feature1, f2 = feature2, corr = corr[feature1, feature2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jets_train = group_features_by_jet(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangzhemeng/anaconda/lib/python3.6/site-packages/numpy/lib/function_base.py:3003: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/xiangzhemeng/anaconda/lib/python3.6/site-packages/numpy/lib/function_base.py:3004: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features 3 and 8 are highly correlated: 0.9999999999988476\n",
      "Features 8 and 3 are highly correlated: 0.9999999999988476\n"
     ]
    }
   ],
   "source": [
    "x,y = replace_missing_data_by_frequent_value(tX[jets_train[0]],tX[jets_train[0]])\n",
    "calculate_correlation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features 0 and 2 are highly correlated: 0.8278513781506542\n",
      "Features 2 and 0 are highly correlated: 0.8278513781506542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangzhemeng/anaconda/lib/python3.6/site-packages/numpy/lib/function_base.py:3003: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/xiangzhemeng/anaconda/lib/python3.6/site-packages/numpy/lib/function_base.py:3004: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features 3 and 9 are highly correlated: 0.8632110677076703\n",
      "Features 3 and 23 are highly correlated: 0.936759033214476\n",
      "Features 3 and 29 are highly correlated: 0.9367590311733179\n",
      "Features 9 and 3 are highly correlated: 0.8632110677076704\n",
      "Features 9 and 23 are highly correlated: 0.9051662981927859\n",
      "Features 9 and 29 are highly correlated: 0.9051663003822761\n",
      "Features 23 and 3 are highly correlated: 0.9367590332144761\n",
      "Features 23 and 9 are highly correlated: 0.9051662981927859\n",
      "Features 23 and 29 are highly correlated: 0.9999999999990182\n",
      "Features 29 and 3 are highly correlated: 0.9367590311733179\n",
      "Features 29 and 9 are highly correlated: 0.9051663003822761\n",
      "Features 29 and 23 are highly correlated: 0.9999999999990182\n"
     ]
    }
   ],
   "source": [
    "x,y = replace_missing_data_by_frequent_value(tX[jets_train[1]],tX[jets_train[1]])\n",
    "calculate_correlation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features 0 and 2 are highly correlated: 0.8127116079837958\n",
      "Features 2 and 0 are highly correlated: 0.8127116079837958\n",
      "Features 9 and 21 are highly correlated: 0.9012851815814814\n",
      "Features 9 and 23 are highly correlated: 0.8789936580551966\n",
      "Features 9 and 29 are highly correlated: 0.9553001014430405\n",
      "Features 21 and 9 are highly correlated: 0.9012851815814814\n",
      "Features 21 and 29 are highly correlated: 0.876350661043983\n",
      "Features 23 and 9 are highly correlated: 0.8789936580551967\n",
      "Features 23 and 29 are highly correlated: 0.881239517517324\n",
      "Features 29 and 9 are highly correlated: 0.9553001014430403\n",
      "Features 29 and 21 are highly correlated: 0.876350661043983\n",
      "Features 29 and 23 are highly correlated: 0.881239517517324\n"
     ]
    }
   ],
   "source": [
    "x,y = replace_missing_data_by_frequent_value(tX[jets_train[2]],tX[jets_train[2]])\n",
    "calculate_correlation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training loss: 0.45082341401577236 / Test loss: 0.44919703102221353\n",
      "1 - Training loss: 0.45045895806996167 / Test loss: 0.4506402874539313\n",
      "2 - Training loss: 0.4504526625472779 / Test loss: 0.4506652766107049\n",
      "3 - Training loss: 0.45036133252034155 / Test loss: 0.4510280209824329\n",
      "4 - Training loss: 0.45038023528767973 / Test loss: 0.45095290877583066\n",
      "Average test loss: 0.45049670496902267\n",
      "Variance test loss: 4.4572066316372895e-07\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "gamma = 0.01\n",
    "max_iters = 500\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    loss_train, loss_test = cross_validation(y, tX, k_indices, k, least_squares_gd, initial_w=None, max_iters=max_iters, gamma=gamma)\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "    \n",
    "for i in range(len(losses_train)):\n",
    "    print(\"{} - Training loss: {} - Test loss: {}\".format(i, losses_train[i], losses_test[i]))\n",
    "print(\"Average test loss: {}\".format(np.mean(losses_test)))\n",
    "print(\"Variance test loss: {}\".format(np.var(losses_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training loss: 0.4514420069441826 / Test loss: 0.4505877245451047\n",
      "1 - Training loss: 0.45047548950479904 / Test loss: 0.45073015408608624\n",
      "2 - Training loss: 0.45046467399687345 / Test loss: 0.45069018416474915\n",
      "3 - Training loss: 0.4520666793653859 / Test loss: 0.45222240628465415\n",
      "4 - Training loss: 0.45049214587484815 / Test loss: 0.4512321791988299\n",
      "Average test loss: 0.45109252965588487\n",
      "Variance test loss: 3.688298677555393e-07\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "gamma = 0.01\n",
    "max_iters = 500\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    loss_train, loss_test = cross_validation(y, tX, k_indices, k, least_squares_sgd, initial_w=None, max_iters=max_iters, gamma=gamma)\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "    \n",
    "for i in range(len(losses_train)):\n",
    "    print(\"{} - Training loss: {} - Test loss: {}\".format(i, losses_train[i], losses_test[i]))\n",
    "print(\"Average test loss: {}\".format(np.mean(losses_test)))\n",
    "print(\"Variance test loss: {}\".format(np.var(losses_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training loss: 0.4508212478 / Test loss: 0.44918415140000006\n",
      "1 - Training loss: 0.4504567758 / Test loss: 0.4506393482\n",
      "2 - Training loss: 0.45045048000000004 / Test loss: 0.4506645440000001\n",
      "3 - Training loss: 0.45035914595000015 / Test loss: 0.4510302876500001\n",
      "4 - Training loss: 0.45037804955000016 / Test loss: 0.45095455445000016\n",
      "Average test loss: 0.4504945771400001\n",
      "Variance test loss: 4.5312557792114635e-07\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    loss_train, loss_test = cross_validation(y, tX, k_indices, k, least_squares)\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "    \n",
    "for i in range(len(losses_train)):\n",
    "    print(\"{} - Training loss: {} - Test loss: {}\".format(i, losses_train[i], losses_test[i]))\n",
    "print(\"Average test loss: {}\".format(np.mean(losses_test)))\n",
    "print(\"Variance test loss: {}\".format(np.var(losses_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training loss: 0.4508220284027237 / Test loss: 0.44919145429913804\n",
      "1 - Training loss: 0.45045756218790195 / Test loss: 0.45063940720782214\n",
      "2 - Training loss: 0.45045126648783357 / Test loss: 0.4506644776432121\n",
      "3 - Training loss: 0.45035993388755735 / Test loss: 0.45102840171624253\n",
      "4 - Training loss: 0.45037883718750493 / Test loss: 0.45095304525523394\n",
      "Average test loss: 0.45049535722432965\n",
      "Variance test loss: 4.486259432284236e-07\n"
     ]
    }
   ],
   "source": [
    "#without grouping\n",
    "k_fold = 5\n",
    "lambda_ = 0.002\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    loss_train, loss_test = cross_validation(y, tX, k_indices, k, ridge_regression, lambda_ = lambda_)\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "\n",
    "for i in range(len(losses_train)):\n",
    "    print(\"{} - Training loss: {} - Test loss: {}\".format(i, losses_train[i], losses_test[i]))\n",
    "print(\"Average test loss: {}\".format(np.mean(losses_test)))\n",
    "print(\"Variance test loss: {}\".format(np.var(losses_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training loss: 0.27641006415678093 - Test loss: 0.28116923876058664\n",
      "1 - Training loss: 0.27615545557588717 - Test loss: 0.27898587725652396\n",
      "2 - Training loss: 0.2753791494242234 - Test loss: 0.2819127993284606\n",
      "3 - Training loss: 0.2760791921759144 - Test loss: 0.27807856503874867\n",
      "4 - Training loss: 0.2761538794294806 - Test loss: 0.2764520940152006\n",
      "Average test loss: 0.2793197148799041\n",
      "Variance test loss: 4.003995090645514e-06\n"
     ]
    }
   ],
   "source": [
    "#group by jets\n",
    "k_fold = 5\n",
    "lambda_ = [0.002, 0.002, 0.001]\n",
    "degree = [1, 4, 5]\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    loss_train, loss_test = cross_validation_ridge_regression(y, tX, k_indices, k, lambda_, degree)\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "\n",
    "for i in range(len(losses_train)):\n",
    "    print(\"{} - Training loss: {} - Test loss: {}\".format(i, losses_train[i], losses_test[i]))\n",
    "print(\"Average test loss: {}\".format(np.mean(losses_test)))\n",
    "print(\"Variance test loss: {}\".format(np.var(losses_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run.py code --> ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "#version 4  best result\n",
    "print('Start!\\n')\n",
    "\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "\n",
    "y, tx_train, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "_, tx_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "# Ridge regression parameters for each subset\n",
    "lambda_ = [0.002,0.002,0.001]\n",
    "# Polynomial features degree for each subset\n",
    "degree = [1,4,5]\n",
    "\n",
    "msks_jet_train = get_jet_masks(tx_train)\n",
    "msks_jet_test = get_jet_masks(tx_test)\n",
    "\n",
    "# Vector to store the final prediction\n",
    "y_pred = np.zeros(tx_test.shape[0])\n",
    "\n",
    "for idx in range(len(msks_jet_train)):\n",
    "    x_train = tx_train[msks_jet_train[idx]]\n",
    "    x_test = tx_test[msks_jet_test[idx]]\n",
    "    y_train = y[msks_jet_train[idx]]\n",
    "\n",
    "    # Pre-processing of data\n",
    "    x_train, x_test = process_data(x_train, x_test)\n",
    "\n",
    "    phi_train = build_poly(x_train, degree[idx])\n",
    "    phi_test = build_poly(x_test, degree[idx])\n",
    "\n",
    "    weights, loss = ridge_regression(y_train, phi_train, lambda_[idx])\n",
    "\n",
    "    y_test_pred = predict_labels(weights, phi_test)\n",
    "\n",
    "    y_pred[msks_jet_test[idx]] = y_test_pred\n",
    "\n",
    "OUTPUT_PATH = 'data/output_ridge_regression.csv'\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "#version 7\n",
    "print('Start!\\n')\n",
    "\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "\n",
    "y, tx_train, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "_, tx_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "# Ridge regression parameters for each subset\n",
    "lambda_ = [0.002,0.002,0.001]\n",
    "# Polynomial features degree for each subset\n",
    "degree = [1,4,5]\n",
    "\n",
    "dict_jet_train = group_features_by_jet(tx_train)\n",
    "dict_jet_test = group_features_by_jet(tx_test)\n",
    "\n",
    "# Vector to store the final prediction\n",
    "y_pred = np.zeros(tx_test.shape[0])\n",
    "\n",
    "for idx in range(len(dict_jet_train)):\n",
    "    x_train = tx_train[dict_jet_train[idx]]\n",
    "    x_test = tx_test[dict_jet_test[idx]]\n",
    "    y_train = y[dict_jet_train[idx]]\n",
    "    \n",
    "    #correlation --> delete columns\n",
    "    if idx == 0:\n",
    "        x_train = np.delete(x_train, [4,5,6,8,12,22,23,24,25,26,27,28], 1)\n",
    "        x_test = np.delete(x_test, [4,5,6,8,12,22,23,24,25,26,27,28], 1)\n",
    "    elif idx == 1:\n",
    "        x_train = np.delete(x_train, [4,5,6,12,22,26,27,28,29], 1)\n",
    "        x_test = np.delete(x_test, [4,5,6,12,22,26,27,28,29], 1)\n",
    "\n",
    "    # Pre-processing of data\n",
    "    x_train, x_test = process_data_ridge_regression(x_train, x_test,idx)\n",
    "\n",
    "    temp_train = build_poly(x_train, degree[idx])\n",
    "    temp_test = build_poly(x_test, degree[idx])\n",
    "\n",
    "    weights, loss = ridge_regression(y_train, temp_train, lambda_[idx])\n",
    "\n",
    "    y_test_pred = predict_labels(weights, temp_test)\n",
    "\n",
    "    y_pred[dict_jet_test[idx]] = y_test_pred\n",
    "\n",
    "OUTPUT_PATH = 'data/output_ridge_regression.csv'\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "\n",
    "print('Finish!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangzhemeng/Desktop/ML-project1/scripts/costs.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n",
      "/Users/xiangzhemeng/Desktop/ML-project1/scripts/costs.py:24: RuntimeWarning: divide by zero encountered in log\n",
      "  pred = sigmoid(np.ones(len(w))+tx.dot(w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training loss: nan / Test loss: 339032417.7031971\n",
      "1 - Training loss: nan / Test loss: 470008119.29838735\n",
      "2 - Training loss: nan / Test loss: 455406651.5480087\n",
      "3 - Training loss: nan / Test loss: 560444754.7400097\n",
      "4 - Training loss: nan / Test loss: 457219234.9831971\n",
      "5 - Training loss: nan / Test loss: 562455338.7031963\n",
      "6 - Training loss: nan / Test loss: 613889940.9031962\n",
      "7 - Training loss: nan / Test loss: 464505568.8200088\n",
      "8 - Training loss: nan / Test loss: 387523905.06319726\n",
      "9 - Training loss: nan / Test loss: 696379747.738406\n",
      "Average test loss: 500686567.9500805\n",
      "Variance test loss: 1.0362501053044878e+16\n",
      "Min test loss: 339032417.7031971\n",
      "Max test loss: 696379747.738406\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "gamma = 0.6\n",
    "max_iters = 100\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "\n",
    "for k in range(k_fold):\n",
    "    loss_train, loss_test = cross_validation(y, tX, k_indices, k, logistic_regression, initial_w=None ,max_iters=max_iters, gamma=gamma)\n",
    "    losses_train.append(loss_train)\n",
    "    losses_test.append(loss_test)\n",
    "    \n",
    "for i in range(len(losses_train)):\n",
    "    print(\"{} - Training loss: {} / Test loss: {}\".format(i, losses_train[i], losses_test[i]))\n",
    "print(\"Average test loss: {}\".format(np.mean(losses_test)))\n",
    "print(\"Variance test loss: {}\".format(np.var(losses_test)))\n",
    "print(\"Min test loss: {}\".format(np.min(losses_test)))\n",
    "print(\"Max test loss: {}\".format(np.max(losses_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
