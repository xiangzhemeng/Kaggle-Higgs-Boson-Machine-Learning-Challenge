{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# native libraries\n",
    "import numpy as np\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# load and organize data\n",
    "path_dataset = \"exerciseData/height_weight_genders.csv\"\n",
    "data = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[1, 2])\n",
    "height = data[:, 0]\n",
    "weight = data[:, 1]\n",
    "gender = np.genfromtxt(\n",
    "    path_dataset, delimiter=\",\", skip_header=1, usecols=[0],\n",
    "    converters={0: lambda x: 0 if b\"Male\" in x else 1})\n",
    "# Convert to metric system\n",
    "height *= 0.025\n",
    "weight *= 0.454\n",
    "\n",
    "x1, mean_x1, std_x2 = standardize(height)\n",
    "x2, mean_x2, std_x2 = standardize(weight)\n",
    "y, tx = build_model_data(x1, x2, gender)\n",
    "\n",
    "m = num_samples = len(y)\n",
    "tx = np.c_[np.ones(m), x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_labels_test(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0)] = 0\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model_data_standardized(x,y):\n",
    "    \"\"\"Form (y,tX) to get regression data in matrix form.\"\"\"\n",
    "    num_samples = len(x[:,0])\n",
    "    num_features = len(x[0,:])\n",
    "    tx = np.ones(num_samples)\n",
    "    for i in range(num_features):\n",
    "        tmp,_,_ = standardize(x[:,i])\n",
    "        tx = np.c_[tx, tmp] \n",
    "    tx = np.nan_to_num(tx)\n",
    "\n",
    "    return tx, y\n",
    "    \n",
    "def load_build_split(loc, degree, split_ratio):\n",
    "    \"\"\"Load the data, build the model and split as train, test samples.\"\"\"\n",
    "    path = loc\n",
    "    y, x, ids = load_csv_data(path)\n",
    "    x[x==-999] = np.nan\n",
    "    x = build_poly(x, degree)[:,1:]\n",
    "    \n",
    "    if split_ratio == 1:\n",
    "        tx_tr, y_tr = build_model_data_standardized(x, y)\n",
    "        tx_te, y_te = 0,0\n",
    "        \n",
    "    else:\n",
    "        x_tr, x_te, y_tr, y_te = split_data(x, y, split_ratio)\n",
    "        tx_tr, y_tr = build_model_data_standardized(x_tr, y_tr)\n",
    "        tx_te, y_te = build_model_data_standardized(x_te, y_te)\n",
    "    \n",
    "    return tx_tr, y_tr, tx_te, y_te\n",
    "    \n",
    "# tx, y, tx_te, y_te = load_build_split(\"data/train.csv\", 5, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required functions\n",
    "def calculate_mse(y, tx, w):\n",
    "    \"\"\"mean square error\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    return 1/2*np.mean(err**2)\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"gradient computation for linear regression\"\"\"\n",
    "    \"\"\"(x transpose times w) is linear predictor\"\"\"\n",
    "    err = tx.dot(w) - y\n",
    "    grad = tx.T.dot(err) / len(err)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# main functions\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"linear regression using gradient descent\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient, loss\n",
    "        grad, _ = compute_gradient(y, tx, w)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # calculate loss\n",
    "        loss = calculate_mse(y, tx, w)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD({bi}/{ti}): loss={l}, weights={},{},{}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, *w.round(5)))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# demo for least_squares_GD\n",
    "# define the parameters\n",
    "max_iters = 200\n",
    "gamma = 0.1\n",
    "# weight initialization\n",
    "n = num_features = len(tx.T)\n",
    "initial_w = np.random.randn(n)\n",
    "# run GD\n",
    "losses, ws = least_squares_GD(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma, batch_size=1):\n",
    "    \"\"\"linear regression using stochastic gradient descent\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = calculate_mse(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, weights={}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, *w.round(5)))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# demo for least_squares_SGD\n",
    "# define the parameters\n",
    "batch_size = 10\n",
    "max_iters = 200\n",
    "gamma = 0.1\n",
    "# weight initialization\n",
    "n = len(tx.T)\n",
    "initial_w = np.random.randn(n)\n",
    "# run SGD\n",
    "losses, ws = least_squares_SGD(y, tx, initial_w, max_iters, gamma, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"least squares regression using normal equations\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = calculate_mse(y, tx, w)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# demo for least_squares\n",
    "_, weight = least_squares(y, tx)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required functions\n",
    "def ridge_mse(y, tx, w):\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    mse = e.dot(e) / (2 * len(e))\n",
    "    return mse\n",
    "\n",
    "# main function\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"rige regression using normal equations\"\"\"\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = ridge_mse(y, tx, w)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# demo for ridge_regression\n",
    "path_dataset = \"exerciseData/dataEx3.csv\"\n",
    "data = np.loadtxt(path_dataset, delimiter=\",\", skiprows=1, unpack=True)\n",
    "x_reg = data[0]\n",
    "y_reg = data[1]\n",
    "\n",
    "seed = 56\n",
    "degree = 3\n",
    "ratio = 0.5\n",
    "# define parameter\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "# split data\n",
    "x_tr, x_te, y_tr, y_te = split_data(x_reg, y_reg, ratio, seed)\n",
    "# form tx\n",
    "tx_tr = build_poly(x_tr, degree)\n",
    "tx_te = build_poly(x_te, degree)\n",
    "\n",
    "# ridge regression with different lambda\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    # ridge regression\n",
    "    _, weight = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    rmse_tr.append(np.sqrt(2 * ridge_mse(y_tr, tx_tr, weight)))\n",
    "    rmse_te.append(np.sqrt(2 * ridge_mse(y_te, tx_te, weight)))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "           p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path = \"data/train.csv\"\n",
    "y, x, ids = load_csv_data(path)\n",
    "x[x==-999] = np.nan\n",
    "\n",
    "# standartization\n",
    "def build_model_data_standardized(x, y):\n",
    "    \"\"\"Form (y,tX) to get regression data in matrix form.\"\"\"\n",
    "    num_samples = len(x[:,0])\n",
    "    num_features = len(x[0,:])\n",
    "    tx = np.ones(num_samples)\n",
    "    for i in range(num_features):\n",
    "        tmp,_,_ = standardize(x[:,i])\n",
    "        tx = np.c_[tx, tmp]\n",
    "    return y, tx\n",
    "\n",
    "y, tx = build_model_data_standardized(x, y)\n",
    "tx = np.nan_to_num(tx)\n",
    "\n",
    "x_reg = tx[:,1:]\n",
    "y_reg = y\n",
    "\n",
    "seed = 56\n",
    "degree = 3\n",
    "ratio = 0.5\n",
    "l = 0.001\n",
    "tx = build_poly(x_reg, degree)\n",
    "_, w = ridge_regression(y_reg, tx, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# required function(s)\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_cost(y, tx, w):    \n",
    "    \"\"\"cost for logistic regression \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    sig = sigmoid(tx.dot(w));\n",
    "    cost = (-y) * np.log(sig + epsilon) - (1-y) * np.log(1-sig + epsilon)\n",
    "    return np.mean(cost)\n",
    "\n",
    "def logistic_gradient(y, tx, w):\n",
    "    \"\"\"gradient for logistic regression \"\"\"\n",
    "    err = sigmoid(tx.dot(w)) - y\n",
    "    grad = tx.T.dot(err) / len(err)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main function(s)\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"logistic regression using GD\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient, loss\n",
    "        grad, _ = logistic_gradient(y, tx, w)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # calculate loss\n",
    "        loss = logistic_cost(y, tx, w)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD({bi}/{ti}): loss={l}, weights={},{},{}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, *w.round(5)))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# demo for logistic_regression\n",
    "# define the parameters\n",
    "max_iters = 300\n",
    "gamma = 0.5\n",
    "# weight initialization\n",
    "n = num_features = len(tx.T)\n",
    "# initial_w = np.zeros(n)\n",
    "initial_w = np.random.randn(n)\n",
    "# run GD\n",
    "losses, ws = logistic_regression(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# accuracy\n",
    "y_pred = predict_labels_test(ws[-1], tx_te)\n",
    "1 - abs(y_pred - y_te).sum() / len(y_te)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0.5156414365231446\n",
    "\n",
    "array([ -8.97096995e-01,   5.78407959e-02,  -6.74223314e-01,\n",
    "        -1.05823773e+00,   3.35177741e-01,   1.15015174e-01,\n",
    "        -5.79652062e-01,   1.93432914e-01,   1.09049192e+00,\n",
    "        -1.33512269e-01,   1.97727558e+00,  -7.77022992e-01,\n",
    "         2.82523310e-01,   3.48718034e-01,   1.32238890e-01,\n",
    "        -2.87410727e-03,  -1.72276007e-03,   6.47910259e-01,\n",
    "        -1.17524165e-03,   6.72229824e-03,   1.36844637e-01,\n",
    "         3.70790487e-04,  -1.89919713e-01,   2.54611379e-01,\n",
    "         1.42883034e+00,   4.66286614e-01,   2.53207178e-01,\n",
    "         8.64995401e-01,  -9.17983499e-01,  -2.80402569e-01,\n",
    "        -1.87980733e+00])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_SGD(y, tx, initial_w, max_iters, gamma, batch_size=1):\n",
    "    \"\"\"logistic regression using stochastic SGD\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = logistic_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = logistic_cost(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, weights={},{},{}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, *w.round(5)))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# demo for logistic_regression_SGD\n",
    "# define the parameters\n",
    "batch_size = 100\n",
    "max_iters = 1000\n",
    "gamma = 0.3\n",
    "# weight initialization\n",
    "n = len(tx.T)\n",
    "# initial_w = np.zeros(n)\n",
    "initial_w = np.random.randn(n)\n",
    "# run SGD\n",
    "losses, ws = logistic_regression_SGD(y, tx, initial_w, max_iters, gamma, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required function(s)\n",
    "def reg_logistic_cost(y, tx, w, alpha):\n",
    "    epsilon = 1e-8\n",
    "    \"\"\"cost for logistic regression with regularization\"\"\"\n",
    "    sig = sigmoid(tx.dot(w));\n",
    "    cost = (-y) * np.log(sig + epsilon) - (1-y) * np.log(1-sig + epsilon)\n",
    "    reg = np.dot(w,w) * alpha / (2 * len(y))\n",
    "    return np.mean(cost) + reg\n",
    "\n",
    "def reg_logistic_gradient(y, tx, w, alpha):\n",
    "    \"\"\"gradient for logistic regression with with regularization\"\"\"\n",
    "    err = sigmoid(tx.dot(w)) - y\n",
    "    grad = tx.T.dot(err) / len(err)\n",
    "    reg = w * alpha / len(err)\n",
    "    return grad - reg, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# main function(s)\n",
    "def reg_logistic_regression(y, tx, alpha, initial_w, max_iters, gamma):\n",
    "    \"\"\"regularized logistic regression using GD\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient, loss\n",
    "        grad, _ = reg_logistic_gradient(y, tx, w, alpha)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # calculate loss\n",
    "        loss = reg_logistic_cost(y, tx, w, alpha)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    '''\n",
    "    print(\"GD({bi}/{ti}): loss={l},\\\n",
    "          \\nweights={},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{}\"\n",
    "          .format(bi=n_iter, ti=max_iters - 1, l=loss, *w.round(2)))\n",
    "    '''\n",
    "    \n",
    "    print(\"GD({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss,))\n",
    "    \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_reg_logistic_regresion(tx_tr, y_tr, tx_te, y_te, alpha, max_iters, gamma):\n",
    "    n = num_features = len(tx_tr.T)\n",
    "    initial_w = np.random.randn(n)\n",
    "#   initial_w = np.zeros(n)\n",
    "    losses, ws = reg_logistic_regression(y_tr, tx_tr, alpha, initial_w, max_iters, gamma)\n",
    "\n",
    "    # accuracy\n",
    "    y_pred = predict_labels_test(ws[-1], tx_te)\n",
    "    return ws[-1], (1 - (abs(y_pred - y_te).sum() / len(y_te)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "degrees = [13,14,15]\n",
    "alphas = [0.001,0.01]\n",
    "gammas = [0.1, 1]\n",
    "\n",
    "max_acc = float()\n",
    "w = list()\n",
    "\n",
    "for degree in degrees:\n",
    "    tx_tr, y_tr, tx_te, y_te = load_build_split(\"data/train.csv\", degree, 0.8)\n",
    "    for gamma in gammas:\n",
    "        for alpha in alphas:\n",
    "            curr_w, acc = test_reg_logistic_regresion(tx_tr, y_tr, tx_te, y_te, alpha, 2000, gamma)\n",
    "            print(\"Degree: %.3f, Gamma: %.3f, Alpha: %.3f > Accuracy: %.5f\"%(degree, gamma, alpha, acc))\n",
    "            if acc > max_acc:\n",
    "                max_acc = acc\n",
    "                w = curr_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression_SGD(y, tx, alpha, initial_w, max_iters, gamma, batch_size=1):\n",
    "    \"\"\"regularized logistic regression using SGD\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = reg_logistic_gradient(y_batch, tx_batch, w, alpha)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = reg_logistic_cost(y, tx, w, alpha)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, weights={},{},{}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, *w.round(5)))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# demo for reg_logistic_regression_SGD\n",
    "# define the parameters\n",
    "batch_size = 50\n",
    "alpha = 0.01\n",
    "max_iters = 500\n",
    "gamma = 0.1\n",
    "# weight initialization\n",
    "n = num_features = len(tx.T)\n",
    "initial_w = np.random.randn(n)\n",
    "# initial_w = np.zeros(n)\n",
    "# run GD\n",
    "losses, ws = reg_logistic_regression_SGD(y, tx, alpha, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tx, y, _, _ = load_build_split(\"data/test.csv\", 3, 1)\n",
    "_,_,ids = load_csv_data(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "y_pred = predict_labels(w, tx)\n",
    "create_csv_submission(ids, y_pred, \"results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
