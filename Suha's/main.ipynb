{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# native libraries\n",
    "import numpy as np\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and organize data\n",
    "path_dataset = \"exerciseData/height_weight_genders.csv\"\n",
    "data = np.genfromtxt(path_dataset, delimiter=\",\", skip_header=1, usecols=[1, 2])\n",
    "height = data[:, 0]\n",
    "weight = data[:, 1]\n",
    "gender = np.genfromtxt(\n",
    "    path_dataset, delimiter=\",\", skip_header=1, usecols=[0],\n",
    "    converters={0: lambda x: 0 if b\"Male\" in x else 1})\n",
    "# Convert to metric system\n",
    "height *= 0.025\n",
    "weight *= 0.454\n",
    "\n",
    "x1, mean_x1, std_x2 = standardize(height)\n",
    "x2, mean_x2, std_x2 = standardize(weight)\n",
    "y, tx = build_model_data(x1, x2, gender)\n",
    "\n",
    "m = num_samples = len(y)\n",
    "tx = np.c_[np.ones(m), x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required functions\n",
    "def calculate_mse(y, tx, w):\n",
    "    \"\"\"mean square error\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    return 1/2*np.mean(err**2)\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"gradient computation for linear regression\"\"\"\n",
    "    \"\"\"(x transpose times w) is linear predictor\"\"\"\n",
    "    err = tx.dot(w) - y\n",
    "    grad = tx.T.dot(err) / len(err)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# main functions\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"linear regression using gradient descent\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient, loss\n",
    "        grad, _ = compute_gradient(y, tx, w)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # calculate loss\n",
    "        loss = calculate_mse(y, tx, w)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD({bi}/{ti}): loss={l}, weights={},{},{}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, *w.round(5)))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD(0/199): loss=2.1861295668691993, weights=-1.56548,0.41748,-0.69178\n",
      "GD(1/199): loss=1.0931332205591207, weights=-0.94584,0.38049,-0.71958\n",
      "GD(2/199): loss=0.5590493310923998, weights=-0.51209,0.36231,-0.72877\n",
      "GD(3/199): loss=0.2975629487071942, weights=-0.20846,0.35214,-0.73016\n",
      "GD(4/199): loss=0.1694248818983369, weights=0.00408,0.3454,-0.72832\n",
      "GD(5/199): loss=0.1065895540532475, weights=0.15285,0.34018,-0.72515\n",
      "GD(6/199): loss=0.07574784463101396, weights=0.257,0.33564,-0.72149\n",
      "GD(7/199): loss=0.060584123643682576, weights=0.3299,0.33145,-0.71767\n",
      "GD(8/199): loss=0.05310468763070618, weights=0.38093,0.32745,-0.71383\n",
      "GD(9/199): loss=0.04939270893461346, weights=0.41665,0.32359,-0.71003\n",
      "GD(10/199): loss=0.04752887773827152, weights=0.44166,0.31984,-0.7063\n",
      "GD(11/199): loss=0.046572644510732696, weights=0.45916,0.31617,-0.70265\n",
      "GD(12/199): loss=0.0460630514761812, weights=0.47141,0.31259,-0.69908\n",
      "GD(13/199): loss=0.04577414393002761, weights=0.47999,0.3091,-0.69559\n",
      "GD(14/199): loss=0.045595122334419765, weights=0.48599,0.30568,-0.69217\n",
      "GD(15/199): loss=0.04547161680471331, weights=0.49019,0.30234,-0.68883\n",
      "GD(16/199): loss=0.045376911468176, weights=0.49314,0.29908,-0.68557\n",
      "GD(17/199): loss=0.04529784424826097, weights=0.4952,0.29589,-0.68238\n",
      "GD(18/199): loss=0.04522789761094603, weights=0.49664,0.29277,-0.67926\n",
      "GD(19/199): loss=0.04516381288851358, weights=0.49765,0.28973,-0.67622\n",
      "GD(20/199): loss=0.04510393116250881, weights=0.49835,0.28675,-0.67324\n",
      "GD(21/199): loss=0.045047380166733736, weights=0.49885,0.28384,-0.67033\n",
      "GD(22/199): loss=0.044993675745842975, weights=0.49919,0.28099,-0.66748\n",
      "GD(23/199): loss=0.04494252645171293, weights=0.49943,0.27821,-0.6647\n",
      "GD(24/199): loss=0.04489373768260234, weights=0.4996,0.27549,-0.66198\n",
      "GD(25/199): loss=0.044847164603327154, weights=0.49972,0.27284,-0.65932\n",
      "GD(26/199): loss=0.044802688972953794, weights=0.49981,0.27024,-0.65673\n",
      "GD(27/199): loss=0.0447602076917827, weights=0.49986,0.2677,-0.65419\n",
      "GD(28/199): loss=0.04471962709518447, weights=0.4999,0.26522,-0.65171\n",
      "GD(29/199): loss=0.04468086006759817, weights=0.49993,0.26279,-0.64928\n",
      "GD(30/199): loss=0.044643824542425374, weights=0.49995,0.26042,-0.64691\n",
      "GD(31/199): loss=0.04460844268485034, weights=0.49997,0.25811,-0.6446\n",
      "GD(32/199): loss=0.04457464041296004, weights=0.49998,0.25584,-0.64233\n",
      "GD(33/199): loss=0.04454234708813322, weights=0.49998,0.25363,-0.64012\n",
      "GD(34/199): loss=0.044511495291717385, weights=0.49999,0.25146,-0.63795\n",
      "GD(35/199): loss=0.04448202064718333, weights=0.49999,0.24935,-0.63584\n",
      "GD(36/199): loss=0.04445386166761773, weights=0.49999,0.24728,-0.63377\n",
      "GD(37/199): loss=0.044426959618548846, weights=0.5,0.24526,-0.63175\n",
      "GD(38/199): loss=0.044401258391072874, weights=0.5,0.24329,-0.62978\n",
      "GD(39/199): loss=0.04437670438269057, weights=0.5,0.24136,-0.62785\n",
      "GD(40/199): loss=0.04435324638446599, weights=0.5,0.23947,-0.62596\n",
      "GD(41/199): loss=0.044330835473713526, weights=0.5,0.23763,-0.62412\n",
      "GD(42/199): loss=0.04430942491171592, weights=0.5,0.23583,-0.62232\n",
      "GD(43/199): loss=0.044288970046125815, weights=0.5,0.23406,-0.62055\n",
      "GD(44/199): loss=0.04426942821778156, weights=0.5,0.23234,-0.61883\n",
      "GD(45/199): loss=0.04425075867171084, weights=0.5,0.23066,-0.61715\n",
      "GD(46/199): loss=0.04423292247212058, weights=0.5,0.22901,-0.6155\n",
      "GD(47/199): loss=0.04421588242118836, weights=0.5,0.22741,-0.6139\n",
      "GD(48/199): loss=0.04419960298148198, weights=0.5,0.22583,-0.61232\n",
      "GD(49/199): loss=0.04418405020184357, weights=0.5,0.2243,-0.61079\n",
      "GD(50/199): loss=0.044169191646582805, weights=0.5,0.2228,-0.60929\n",
      "GD(51/199): loss=0.0441549963278311, weights=0.5,0.22133,-0.60782\n",
      "GD(52/199): loss=0.044141434640915495, weights=0.5,0.2199,-0.60638\n",
      "GD(53/199): loss=0.044128478302617284, weights=0.5,0.21849,-0.60498\n",
      "GD(54/199): loss=0.04411610029218667, weights=0.5,0.21712,-0.60361\n",
      "GD(55/199): loss=0.044104274794990295, weights=0.5,0.21578,-0.60227\n",
      "GD(56/199): loss=0.044092977148674245, weights=0.5,0.21447,-0.60096\n",
      "GD(57/199): loss=0.044082183791730074, weights=0.5,0.21319,-0.59968\n",
      "GD(58/199): loss=0.04407187221435677, weights=0.5,0.21194,-0.59843\n",
      "GD(59/199): loss=0.044062020911515995, weights=0.5,0.21072,-0.59721\n",
      "GD(60/199): loss=0.044052609338082804, weights=0.5,0.20953,-0.59602\n",
      "GD(61/199): loss=0.04404361786599837, weights=0.5,0.20836,-0.59485\n",
      "GD(62/199): loss=0.04403502774333514, weights=0.5,0.20722,-0.59371\n",
      "GD(63/199): loss=0.04402682105518922, weights=0.5,0.2061,-0.59259\n",
      "GD(64/199): loss=0.04401898068631856, weights=0.5,0.20501,-0.5915\n",
      "GD(65/199): loss=0.04401149028544852, weights=0.5,0.20394,-0.59043\n",
      "GD(66/199): loss=0.04400433423117106, weights=0.5,0.2029,-0.58939\n",
      "GD(67/199): loss=0.04399749759936583, weights=0.5,0.20188,-0.58837\n",
      "GD(68/199): loss=0.04399096613207563, weights=0.5,0.20089,-0.58738\n",
      "GD(69/199): loss=0.04398472620777103, weights=0.5,0.19992,-0.58641\n",
      "GD(70/199): loss=0.04397876481294239, weights=0.5,0.19896,-0.58545\n",
      "GD(71/199): loss=0.043973069514959925, weights=0.5,0.19804,-0.58452\n",
      "GD(72/199): loss=0.043967628436145045, weights=0.5,0.19713,-0.58362\n",
      "GD(73/199): loss=0.04396243022899929, weights=0.5,0.19624,-0.58273\n",
      "GD(74/199): loss=0.04395746405253878, weights=0.5,0.19537,-0.58186\n",
      "GD(75/199): loss=0.04395271954968514, weights=0.5,0.19452,-0.58101\n",
      "GD(76/199): loss=0.04394818682566552, weights=0.5,0.19369,-0.58018\n",
      "GD(77/199): loss=0.04394385642737683, weights=0.5,0.19288,-0.57937\n",
      "GD(78/199): loss=0.0439397193236711, weights=0.5,0.19209,-0.57858\n",
      "GD(79/199): loss=0.04393576688652075, weights=0.5,0.19132,-0.57781\n",
      "GD(80/199): loss=0.04393199087302469, weights=0.5,0.19056,-0.57705\n",
      "GD(81/199): loss=0.04392838340821753, weights=0.5,0.18982,-0.57631\n",
      "GD(82/199): loss=0.04392493696864613, weights=0.5,0.1891,-0.57559\n",
      "GD(83/199): loss=0.043921644366679335, weights=0.5,0.18839,-0.57488\n",
      "GD(84/199): loss=0.04391849873551788, weights=0.5,0.1877,-0.57419\n",
      "GD(85/199): loss=0.043915493514873644, weights=0.5,0.18702,-0.57351\n",
      "GD(86/199): loss=0.04391262243728795, weights=0.5,0.18636,-0.57285\n",
      "GD(87/199): loss=0.043909879515060604, weights=0.5,0.18572,-0.57221\n",
      "GD(88/199): loss=0.0439072590277625, weights=0.5,0.18509,-0.57158\n",
      "GD(89/199): loss=0.04390475551030541, weights=0.5,0.18447,-0.57096\n",
      "GD(90/199): loss=0.043902363741544534, weights=0.5,0.18387,-0.57036\n",
      "GD(91/199): loss=0.043900078733389566, weights=0.5,0.18328,-0.56977\n",
      "GD(92/199): loss=0.04389789572040201, weights=0.5,0.1827,-0.56919\n",
      "GD(93/199): loss=0.043895810149856655, weights=0.5,0.18214,-0.56863\n",
      "GD(94/199): loss=0.04389381767224685, weights=0.5,0.18159,-0.56808\n",
      "GD(95/199): loss=0.04389191413221344, weights=0.5,0.18106,-0.56754\n",
      "GD(96/199): loss=0.043890095559878696, weights=0.5,0.18053,-0.56702\n",
      "GD(97/199): loss=0.043888358162567025, weights=0.5,0.18002,-0.56651\n",
      "GD(98/199): loss=0.043886698316895276, weights=0.5,0.17951,-0.566\n",
      "GD(99/199): loss=0.04388511256121606, weights=0.5,0.17902,-0.56551\n",
      "GD(100/199): loss=0.04388359758839843, weights=0.5,0.17854,-0.56503\n",
      "GD(101/199): loss=0.043882150238930766, weights=0.5,0.17808,-0.56457\n",
      "GD(102/199): loss=0.04388076749433155, weights=0.5,0.17762,-0.56411\n",
      "GD(103/199): loss=0.04387944647085422, weights=0.5,0.17717,-0.56366\n",
      "GD(104/199): loss=0.043878184413473104, weights=0.5,0.17673,-0.56322\n",
      "GD(105/199): loss=0.04387697869013771, weights=0.5,0.17631,-0.56279\n",
      "GD(106/199): loss=0.04387582678628349, weights=0.5,0.17589,-0.56238\n",
      "GD(107/199): loss=0.043874726299587774, weights=0.5,0.17548,-0.56197\n",
      "GD(108/199): loss=0.043873674934959514, weights=0.5,0.17508,-0.56157\n",
      "GD(109/199): loss=0.04387267049975294, weights=0.5,0.17469,-0.56118\n",
      "GD(110/199): loss=0.04387171089919468, weights=0.5,0.17431,-0.5608\n",
      "GD(111/199): loss=0.043870794132015105, weights=0.5,0.17393,-0.56042\n",
      "GD(112/199): loss=0.043869918286274574, weights=0.5,0.17357,-0.56006\n",
      "GD(113/199): loss=0.04386908153537611, weights=0.5,0.17321,-0.5597\n",
      "GD(114/199): loss=0.04386828213425591, weights=0.5,0.17287,-0.55935\n",
      "GD(115/199): loss=0.04386751841574408, weights=0.5,0.17252,-0.55901\n",
      "GD(116/199): loss=0.043866788787087586, weights=0.5,0.17219,-0.55868\n",
      "GD(117/199): loss=0.04386609172662862, weights=0.5,0.17187,-0.55836\n",
      "GD(118/199): loss=0.04386542578063114, weights=0.5,0.17155,-0.55804\n",
      "GD(119/199): loss=0.043864789560249, weights=0.5,0.17124,-0.55773\n",
      "GD(120/199): loss=0.04386418173862957, weights=0.5,0.17093,-0.55742\n",
      "GD(121/199): loss=0.043863601048146496, weights=0.5,0.17064,-0.55713\n",
      "GD(122/199): loss=0.04386304627775605, weights=0.5,0.17035,-0.55684\n",
      "GD(123/199): loss=0.04386251627047146, weights=0.5,0.17006,-0.55655\n",
      "GD(124/199): loss=0.043862009920950025, weights=0.5,0.16979,-0.55628\n",
      "GD(125/199): loss=0.04386152617318784, weights=0.5,0.16952,-0.55601\n",
      "GD(126/199): loss=0.0438610640183175, weights=0.5,0.16925,-0.55574\n",
      "GD(127/199): loss=0.04386062249250412, weights=0.5,0.16899,-0.55548\n",
      "GD(128/199): loss=0.043860200674935165, weights=0.5,0.16874,-0.55523\n",
      "GD(129/199): loss=0.04385979768590015, weights=0.5,0.16849,-0.55498\n",
      "GD(130/199): loss=0.043859412684955884, weights=0.5,0.16825,-0.55474\n",
      "GD(131/199): loss=0.04385904486917381, weights=0.5,0.16801,-0.5545\n",
      "GD(132/199): loss=0.043858693471465426, weights=0.5,0.16778,-0.55427\n",
      "GD(133/199): loss=0.0438583577589825, weights=0.5,0.16756,-0.55405\n",
      "GD(134/199): loss=0.04385803703158873, weights=0.5,0.16734,-0.55383\n",
      "GD(135/199): loss=0.04385773062039953, weights=0.5,0.16712,-0.55361\n",
      "GD(136/199): loss=0.04385743788638714, weights=0.5,0.16691,-0.5534\n",
      "GD(137/199): loss=0.04385715821904785, weights=0.5,0.1667,-0.55319\n",
      "GD(138/199): loss=0.04385689103512881, weights=0.5,0.1665,-0.55299\n",
      "GD(139/199): loss=0.043856635777411654, weights=0.5,0.16631,-0.5528\n",
      "GD(140/199): loss=0.04385639191355036, weights=0.5,0.16611,-0.5526\n",
      "GD(141/199): loss=0.043856158934961115, weights=0.5,0.16593,-0.55242\n",
      "GD(142/199): loss=0.043855936355761536, weights=0.5,0.16574,-0.55223\n",
      "GD(143/199): loss=0.043855723711757444, weights=0.5,0.16556,-0.55205\n",
      "GD(144/199): loss=0.04385552055947476, weights=0.5,0.16539,-0.55188\n",
      "GD(145/199): loss=0.043855326475234574, weights=0.5,0.16522,-0.55171\n",
      "GD(146/199): loss=0.043855141054269624, weights=0.5,0.16505,-0.55154\n",
      "GD(147/199): loss=0.04385496390988011, weights=0.5,0.16488,-0.55137\n",
      "GD(148/199): loss=0.04385479467262722, weights=0.5,0.16472,-0.55121\n",
      "GD(149/199): loss=0.043854632989562665, weights=0.5,0.16457,-0.55106\n",
      "GD(150/199): loss=0.04385447852349257, weights=0.5,0.16441,-0.5509\n",
      "GD(151/199): loss=0.043854330952274326, weights=0.5,0.16426,-0.55075\n",
      "GD(152/199): loss=0.04385418996814465, weights=0.5,0.16412,-0.55061\n",
      "GD(153/199): loss=0.043854055277077815, weights=0.5,0.16398,-0.55046\n",
      "GD(154/199): loss=0.043853926598172416, weights=0.5,0.16384,-0.55033\n",
      "GD(155/199): loss=0.04385380366306555, weights=0.5,0.1637,-0.55019\n",
      "GD(156/199): loss=0.04385368621537315, weights=0.5,0.16357,-0.55006\n",
      "GD(157/199): loss=0.043853574010155286, weights=0.5,0.16344,-0.54992\n",
      "GD(158/199): loss=0.04385346681340531, weights=0.5,0.16331,-0.5498\n",
      "GD(159/199): loss=0.04385336440156187, weights=0.5,0.16318,-0.54967\n",
      "GD(160/199): loss=0.043853266561042645, weights=0.5,0.16306,-0.54955\n",
      "GD(161/199): loss=0.04385317308779893, weights=0.5,0.16294,-0.54943\n",
      "GD(162/199): loss=0.04385308378689005, weights=0.5,0.16283,-0.54932\n",
      "GD(163/199): loss=0.04385299847207686, weights=0.5,0.16271,-0.5492\n",
      "GD(164/199): loss=0.043852916965433295, weights=0.5,0.1626,-0.54909\n",
      "GD(165/199): loss=0.04385283909697532, weights=0.5,0.16249,-0.54898\n",
      "GD(166/199): loss=0.04385276470430644, weights=0.5,0.16239,-0.54888\n",
      "GD(167/199): loss=0.04385269363227897, weights=0.5,0.16228,-0.54877\n",
      "GD(168/199): loss=0.043852625732670535, weights=0.5,0.16218,-0.54867\n",
      "GD(169/199): loss=0.043852560863874866, weights=0.5,0.16208,-0.54857\n",
      "GD(170/199): loss=0.043852498890606585, weights=0.5,0.16198,-0.54847\n",
      "GD(171/199): loss=0.043852439683618974, weights=0.5,0.16189,-0.54838\n",
      "GD(172/199): loss=0.04385238311943444, weights=0.5,0.1618,-0.54829\n",
      "GD(173/199): loss=0.04385232908008706, weights=0.5,0.16171,-0.5482\n",
      "GD(174/199): loss=0.04385227745287648, weights=0.5,0.16162,-0.54811\n",
      "GD(175/199): loss=0.04385222813013296, weights=0.5,0.16153,-0.54802\n",
      "GD(176/199): loss=0.04385218100899272, weights=0.5,0.16145,-0.54794\n",
      "GD(177/199): loss=0.04385213599118354, weights=0.5,0.16136,-0.54785\n",
      "GD(178/199): loss=0.043852092982819675, weights=0.5,0.16128,-0.54777\n",
      "GD(179/199): loss=0.043852051894206205, weights=0.5,0.1612,-0.54769\n",
      "GD(180/199): loss=0.043852012639651834, weights=0.5,0.16113,-0.54762\n",
      "GD(181/199): loss=0.043851975137290264, weights=0.5,0.16105,-0.54754\n",
      "GD(182/199): loss=0.04385193930890944, weights=0.5,0.16098,-0.54747\n",
      "GD(183/199): loss=0.043851905079788416, weights=0.5,0.16091,-0.5474\n",
      "GD(184/199): loss=0.043851872378541566, weights=0.5,0.16084,-0.54732\n",
      "GD(185/199): loss=0.04385184113696965, weights=0.5,0.16077,-0.54726\n",
      "GD(186/199): loss=0.04385181128991765, weights=0.5,0.1607,-0.54719\n",
      "GD(187/199): loss=0.04385178277513882, weights=0.5,0.16063,-0.54712\n",
      "GD(188/199): loss=0.04385175553316492, weights=0.5,0.16057,-0.54706\n",
      "GD(189/199): loss=0.043851729507182156, weights=0.5,0.16051,-0.547\n",
      "GD(190/199): loss=0.04385170464291274, weights=0.5,0.16045,-0.54693\n",
      "GD(191/199): loss=0.043851680888501665, weights=0.5,0.16039,-0.54687\n",
      "GD(192/199): loss=0.04385165819440851, weights=0.5,0.16033,-0.54682\n",
      "GD(193/199): loss=0.04385163651330423, weights=0.5,0.16027,-0.54676\n",
      "GD(194/199): loss=0.04385161579997237, weights=0.5,0.16021,-0.5467\n",
      "GD(195/199): loss=0.04385159601121478, weights=0.5,0.16016,-0.54665\n",
      "GD(196/199): loss=0.04385157710576154, weights=0.5,0.1601,-0.54659\n",
      "GD(197/199): loss=0.04385155904418487, weights=0.5,0.16005,-0.54654\n",
      "GD(198/199): loss=0.04385154178881694, weights=0.5,0.16,-0.54649\n",
      "GD(199/199): loss=0.04385152530367128, weights=0.5,0.15995,-0.54644\n"
     ]
    }
   ],
   "source": [
    "# demo for least_squares_GD\n",
    "# define the parameters\n",
    "max_iters = 200\n",
    "gamma = 0.3\n",
    "# weight initialization\n",
    "n = num_features = len(tx.T)\n",
    "initial_w = np.random.randn(n)\n",
    "# run GD\n",
    "losses, ws = least_squares_GD(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma, batch_size=1):\n",
    "    \"\"\"linear regression using stochastic gradient descent\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = calculate_mse(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, weights={},{},{}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, *w.round(5)))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/199): loss=1.8809274524530604, weights=-0.91219,1.26361,-0.34101\n",
      "SGD(1/199): loss=0.7877918522795685, weights=-0.61997,0.84701,-0.77574\n",
      "SGD(2/199): loss=0.3370006969139371, weights=-0.24555,0.61632,-0.97114\n",
      "SGD(3/199): loss=0.18386796507735204, weights=-0.00114,0.58307,-0.98947\n",
      "SGD(4/199): loss=0.13792111727396136, weights=0.10879,0.62544,-0.91825\n",
      "SGD(5/199): loss=0.0944130529019627, weights=0.22618,0.56106,-0.96812\n",
      "SGD(6/199): loss=0.07919437908857851, weights=0.29289,0.55218,-0.98156\n",
      "SGD(7/199): loss=0.05987924440133609, weights=0.42928,0.522,-0.96965\n",
      "SGD(8/199): loss=0.0705257229935633, weights=0.36183,0.59599,-0.86921\n",
      "SGD(9/199): loss=0.05821440578071698, weights=0.40398,0.51936,-0.90258\n",
      "SGD(10/199): loss=0.05584724569333515, weights=0.44024,0.49637,-0.91918\n",
      "SGD(11/199): loss=0.0627370204238695, weights=0.37742,0.4702,-0.92598\n",
      "SGD(12/199): loss=0.0628961442710562, weights=0.36401,0.47033,-0.90713\n",
      "SGD(13/199): loss=0.056922492174512, weights=0.40039,0.47522,-0.87817\n",
      "SGD(14/199): loss=0.0611898129807577, weights=0.39495,0.42549,-0.907\n",
      "SGD(15/199): loss=0.051803309087970956, weights=0.45969,0.46105,-0.8557\n",
      "SGD(16/199): loss=0.05097564065163911, weights=0.49693,0.43655,-0.8567\n",
      "SGD(17/199): loss=0.05352610256222153, weights=0.4418,0.47393,-0.79806\n",
      "SGD(18/199): loss=0.05171687027558354, weights=0.4608,0.38543,-0.83658\n",
      "SGD(19/199): loss=0.05547061436631454, weights=0.48927,0.34657,-0.85288\n",
      "SGD(20/199): loss=0.05673581274610997, weights=0.47351,0.34062,-0.85557\n",
      "SGD(21/199): loss=0.04979295171816948, weights=0.46698,0.38623,-0.81241\n",
      "SGD(22/199): loss=0.0487946572438814, weights=0.50152,0.38248,-0.80278\n",
      "SGD(23/199): loss=0.04925399674396897, weights=0.45665,0.38091,-0.79204\n",
      "SGD(24/199): loss=0.055406810874967787, weights=0.45272,0.31583,-0.8218\n",
      "SGD(25/199): loss=0.053907985500679705, weights=0.46794,0.32217,-0.81947\n",
      "SGD(26/199): loss=0.04830217136561669, weights=0.48468,0.35052,-0.77984\n",
      "SGD(27/199): loss=0.0489691651064328, weights=0.51721,0.34243,-0.78575\n",
      "SGD(28/199): loss=0.04749060187032198, weights=0.46541,0.34451,-0.74909\n",
      "SGD(29/199): loss=0.0495982951237267, weights=0.42469,0.35808,-0.73305\n",
      "SGD(30/199): loss=0.04830735840205107, weights=0.48362,0.31707,-0.76207\n",
      "SGD(31/199): loss=0.048567106668246034, weights=0.47532,0.30937,-0.75858\n",
      "SGD(32/199): loss=0.04886341765821514, weights=0.48063,0.30184,-0.759\n",
      "SGD(33/199): loss=0.047927793051828434, weights=0.48542,0.30231,-0.74804\n",
      "SGD(34/199): loss=0.05027517500355944, weights=0.47013,0.37627,-0.67531\n",
      "SGD(35/199): loss=0.04824585510171831, weights=0.46853,0.3598,-0.68763\n",
      "SGD(36/199): loss=0.047346500083889215, weights=0.47901,0.35276,-0.69226\n",
      "SGD(37/199): loss=0.0477374368871608, weights=0.52277,0.30023,-0.7417\n",
      "SGD(38/199): loss=0.04791447625925621, weights=0.51805,0.29513,-0.74248\n",
      "SGD(39/199): loss=0.047037385655963, weights=0.53381,0.3182,-0.73138\n",
      "SGD(40/199): loss=0.04774919157599219, weights=0.5353,0.30245,-0.73738\n",
      "SGD(41/199): loss=0.04721487898569379, weights=0.47881,0.29308,-0.72962\n",
      "SGD(42/199): loss=0.053461340041490224, weights=0.52012,0.37204,-0.63211\n",
      "SGD(43/199): loss=0.046270755795459764, weights=0.49829,0.3257,-0.67212\n",
      "SGD(44/199): loss=0.04710609456008546, weights=0.46521,0.32442,-0.66262\n",
      "SGD(45/199): loss=0.04662153396236899, weights=0.46892,0.27162,-0.70149\n",
      "SGD(46/199): loss=0.04556177356000654, weights=0.52436,0.28825,-0.68397\n",
      "SGD(47/199): loss=0.05594446211173267, weights=0.57005,0.35122,-0.60542\n",
      "SGD(48/199): loss=0.04886154924556855, weights=0.56037,0.3189,-0.64211\n",
      "SGD(49/199): loss=0.04664670858456189, weights=0.51191,0.25012,-0.69458\n",
      "SGD(50/199): loss=0.044864000771513146, weights=0.50744,0.27442,-0.65182\n",
      "SGD(51/199): loss=0.04573963047115078, weights=0.5085,0.29441,-0.63897\n",
      "SGD(52/199): loss=0.04795902338864289, weights=0.50755,0.31191,-0.61811\n",
      "SGD(53/199): loss=0.0453559734181035, weights=0.49235,0.28858,-0.64346\n",
      "SGD(54/199): loss=0.0449315407672141, weights=0.4962,0.27017,-0.66601\n",
      "SGD(55/199): loss=0.044943767972322926, weights=0.48928,0.27734,-0.65423\n",
      "SGD(56/199): loss=0.04500264103816991, weights=0.51341,0.26024,-0.66357\n",
      "SGD(57/199): loss=0.04574860846610923, weights=0.52046,0.281,-0.6239\n",
      "SGD(58/199): loss=0.04687227573407419, weights=0.56465,0.22948,-0.64401\n",
      "SGD(59/199): loss=0.046958099961379195, weights=0.51174,0.27019,-0.58306\n",
      "SGD(60/199): loss=0.04471043927838914, weights=0.4689,0.21156,-0.61224\n",
      "SGD(61/199): loss=0.04507854465278754, weights=0.46098,0.20774,-0.61436\n",
      "SGD(62/199): loss=0.047572863390462544, weights=0.43542,0.1849,-0.62562\n",
      "SGD(63/199): loss=0.04469685442744052, weights=0.46374,0.20496,-0.59524\n",
      "SGD(64/199): loss=0.04563612936794015, weights=0.46201,0.17945,-0.60968\n",
      "SGD(65/199): loss=0.04712390688208667, weights=0.49766,0.15794,-0.62529\n",
      "SGD(66/199): loss=0.04401292970311297, weights=0.50913,0.19755,-0.58438\n",
      "SGD(67/199): loss=0.045637266037459524, weights=0.55227,0.21673,-0.58043\n",
      "SGD(68/199): loss=0.04630317569665948, weights=0.52387,0.16634,-0.61794\n",
      "SGD(69/199): loss=0.043982896019531904, weights=0.49907,0.20031,-0.58442\n",
      "SGD(70/199): loss=0.04407721522042249, weights=0.50629,0.20415,-0.57709\n",
      "SGD(71/199): loss=0.044878399789753845, weights=0.48054,0.17555,-0.60108\n",
      "SGD(72/199): loss=0.04408170481687215, weights=0.49339,0.18334,-0.58587\n",
      "SGD(73/199): loss=0.0469644686414455, weights=0.51815,0.14697,-0.61096\n",
      "SGD(74/199): loss=0.04414158170300178, weights=0.48045,0.19107,-0.56886\n",
      "SGD(75/199): loss=0.046558948577045706, weights=0.46688,0.21488,-0.53505\n",
      "SGD(76/199): loss=0.04391167759198124, weights=0.49281,0.17674,-0.56596\n",
      "SGD(77/199): loss=0.045172107579825135, weights=0.52662,0.15251,-0.58332\n",
      "SGD(78/199): loss=0.04595656765976437, weights=0.50082,0.14081,-0.59314\n",
      "SGD(79/199): loss=0.05239165580086335, weights=0.52583,0.22919,-0.4851\n",
      "SGD(80/199): loss=0.04948957046124016, weights=0.56663,0.21179,-0.51412\n",
      "SGD(81/199): loss=0.046654129839004105, weights=0.56471,0.18803,-0.53638\n",
      "SGD(82/199): loss=0.04396849153890546, weights=0.508,0.18018,-0.55509\n",
      "SGD(83/199): loss=0.04406816200819552, weights=0.50197,0.16963,-0.57547\n",
      "SGD(84/199): loss=0.04437660737645945, weights=0.49456,0.16292,-0.58092\n",
      "SGD(85/199): loss=0.044080202108869954, weights=0.49359,0.16222,-0.56873\n",
      "SGD(86/199): loss=0.04414274048000832, weights=0.51776,0.18476,-0.55647\n",
      "SGD(87/199): loss=0.04521179990606491, weights=0.52907,0.15659,-0.58648\n",
      "SGD(88/199): loss=0.04453062409494102, weights=0.522,0.19302,-0.55049\n",
      "SGD(89/199): loss=0.044084748115094656, weights=0.4923,0.18768,-0.55524\n",
      "SGD(90/199): loss=0.04527770392343449, weights=0.49871,0.14702,-0.58756\n",
      "SGD(91/199): loss=0.044113516656432976, weights=0.51812,0.17642,-0.54941\n",
      "SGD(92/199): loss=0.045770915401078506, weights=0.52004,0.13939,-0.58548\n",
      "SGD(93/199): loss=0.04676774069074867, weights=0.5616,0.14059,-0.57305\n",
      "SGD(94/199): loss=0.04432038575031388, weights=0.47986,0.17139,-0.57935\n",
      "SGD(95/199): loss=0.04426855658138519, weights=0.47425,0.17452,-0.57121\n",
      "SGD(96/199): loss=0.04545808103763549, weights=0.44466,0.18986,-0.57238\n",
      "SGD(97/199): loss=0.04483871773605373, weights=0.45779,0.17922,-0.57538\n",
      "SGD(98/199): loss=0.043918009488627355, weights=0.49043,0.17273,-0.56122\n",
      "SGD(99/199): loss=0.04388606125619906, weights=0.50548,0.17245,-0.56078\n",
      "SGD(100/199): loss=0.044635939645190546, weights=0.46291,0.17805,-0.55141\n",
      "SGD(101/199): loss=0.04575767049703913, weights=0.47282,0.1916,-0.52162\n",
      "SGD(102/199): loss=0.0445547591003902, weights=0.46354,0.16766,-0.54543\n",
      "SGD(103/199): loss=0.04397943386788704, weights=0.48562,0.15898,-0.55241\n",
      "SGD(104/199): loss=0.044832756963578364, weights=0.4589,0.1568,-0.55992\n",
      "SGD(105/199): loss=0.047657267372759445, weights=0.44411,0.12975,-0.5845\n",
      "SGD(106/199): loss=0.043880062019194935, weights=0.50376,0.16088,-0.55364\n",
      "SGD(107/199): loss=0.04559892534750432, weights=0.47457,0.13594,-0.57679\n",
      "SGD(108/199): loss=0.04546514948816244, weights=0.5071,0.12904,-0.57299\n",
      "SGD(109/199): loss=0.04505292228248458, weights=0.49859,0.1864,-0.52296\n",
      "SGD(110/199): loss=0.04641475385621033, weights=0.47066,0.18728,-0.50721\n",
      "SGD(111/199): loss=0.045642709539594144, weights=0.4491,0.12764,-0.54573\n",
      "SGD(112/199): loss=0.046094957399992253, weights=0.5201,0.18303,-0.50444\n",
      "SGD(113/199): loss=0.04705860765578703, weights=0.55506,0.17279,-0.50027\n",
      "SGD(114/199): loss=0.04475741021678395, weights=0.53854,0.13279,-0.53656\n",
      "SGD(115/199): loss=0.04988256315704738, weights=0.48635,0.19679,-0.47238\n",
      "SGD(116/199): loss=0.04431149575146435, weights=0.49123,0.15928,-0.51662\n",
      "SGD(117/199): loss=0.04578033773468354, weights=0.51206,0.18167,-0.50611\n",
      "SGD(118/199): loss=0.04633923194478721, weights=0.47826,0.18903,-0.50712\n",
      "SGD(119/199): loss=0.044832335464360155, weights=0.47752,0.1792,-0.52679\n",
      "SGD(120/199): loss=0.04462610115840985, weights=0.47432,0.14138,-0.55828\n",
      "SGD(121/199): loss=0.04602387944004955, weights=0.43513,0.14961,-0.54801\n",
      "SGD(122/199): loss=0.045779486678834726, weights=0.43903,0.15393,-0.55241\n",
      "SGD(123/199): loss=0.04551351518628719, weights=0.47433,0.12941,-0.56852\n",
      "SGD(124/199): loss=0.045697784585980686, weights=0.45808,0.13653,-0.56788\n",
      "SGD(125/199): loss=0.04600603333449031, weights=0.48626,0.19773,-0.51884\n",
      "SGD(126/199): loss=0.04394974431839222, weights=0.48602,0.1601,-0.54731\n",
      "SGD(127/199): loss=0.04456936726930631, weights=0.51304,0.14263,-0.56537\n",
      "SGD(128/199): loss=0.04775265507972886, weights=0.50718,0.11787,-0.59409\n",
      "SGD(129/199): loss=0.04575429917850432, weights=0.51108,0.14211,-0.59017\n",
      "SGD(130/199): loss=0.04815187269328866, weights=0.52291,0.12639,-0.60432\n",
      "SGD(131/199): loss=0.04413368070224364, weights=0.4792,0.18123,-0.55869\n",
      "SGD(132/199): loss=0.044164438949031624, weights=0.47693,0.17268,-0.56594\n",
      "SGD(133/199): loss=0.046539478992049084, weights=0.4735,0.20199,-0.51888\n",
      "SGD(134/199): loss=0.04767237666173379, weights=0.5264,0.20961,-0.51123\n",
      "SGD(135/199): loss=0.04476593363935146, weights=0.53721,0.17752,-0.5428\n",
      "SGD(136/199): loss=0.04571053665423486, weights=0.51201,0.19908,-0.52478\n",
      "SGD(137/199): loss=0.04673395252135257, weights=0.51659,0.20965,-0.52082\n",
      "SGD(138/199): loss=0.04642707970997129, weights=0.56491,0.18731,-0.54307\n",
      "SGD(139/199): loss=0.04607012674922163, weights=0.5623,0.17874,-0.54145\n",
      "SGD(140/199): loss=0.044770900187093954, weights=0.54131,0.1651,-0.56221\n",
      "SGD(141/199): loss=0.04552496924057011, weights=0.53907,0.18928,-0.53244\n",
      "SGD(142/199): loss=0.04397823307681533, weights=0.49236,0.18191,-0.55602\n",
      "SGD(143/199): loss=0.045263575968323304, weights=0.45437,0.19746,-0.55828\n",
      "SGD(144/199): loss=0.044191154098313236, weights=0.47704,0.18639,-0.56488\n",
      "SGD(145/199): loss=0.044544638729072925, weights=0.46895,0.19161,-0.55952\n",
      "SGD(146/199): loss=0.04698675565893191, weights=0.4923,0.13243,-0.59905\n",
      "SGD(147/199): loss=0.04534749624243319, weights=0.45488,0.18374,-0.53896\n",
      "SGD(148/199): loss=0.04718389877579685, weights=0.53001,0.21346,-0.52285\n",
      "SGD(149/199): loss=0.04414541212651175, weights=0.51574,0.18457,-0.55365\n",
      "SGD(150/199): loss=0.046411530583143235, weights=0.55172,0.19605,-0.5324\n",
      "SGD(151/199): loss=0.04449191436533479, weights=0.51283,0.19196,-0.5451\n",
      "SGD(152/199): loss=0.044041130014886236, weights=0.49514,0.16272,-0.56763\n",
      "SGD(153/199): loss=0.04467234442838521, weights=0.47463,0.16477,-0.58222\n",
      "SGD(154/199): loss=0.049268561452079224, weights=0.53558,0.115,-0.60116\n",
      "SGD(155/199): loss=0.04391672954134866, weights=0.51023,0.1626,-0.55353\n",
      "SGD(156/199): loss=0.0439363624765967, weights=0.49088,0.16654,-0.5611\n",
      "SGD(157/199): loss=0.0450633310043591, weights=0.4984,0.13953,-0.57611\n",
      "SGD(158/199): loss=0.04488955140498436, weights=0.53778,0.17593,-0.53651\n",
      "SGD(159/199): loss=0.045173234091313214, weights=0.54995,0.15754,-0.55626\n",
      "SGD(160/199): loss=0.04464996200461535, weights=0.52997,0.14782,-0.56123\n",
      "SGD(161/199): loss=0.04573199273958926, weights=0.49074,0.19509,-0.51982\n",
      "SGD(162/199): loss=0.04521616216467104, weights=0.54002,0.18238,-0.53476\n",
      "SGD(163/199): loss=0.0454753097320682, weights=0.48436,0.1907,-0.52135\n",
      "SGD(164/199): loss=0.043869247698924634, weights=0.50167,0.16395,-0.54469\n",
      "SGD(165/199): loss=0.04392540940640074, weights=0.51163,0.15586,-0.53894\n",
      "SGD(166/199): loss=0.0477086565249284, weights=0.45525,0.19489,-0.50434\n",
      "SGD(167/199): loss=0.04622523437391177, weights=0.45338,0.18924,-0.52406\n",
      "SGD(168/199): loss=0.04480413753507399, weights=0.46266,0.17836,-0.54207\n",
      "SGD(169/199): loss=0.045400503310076404, weights=0.44837,0.1752,-0.54065\n",
      "SGD(170/199): loss=0.0499443429862134, weights=0.41234,0.19274,-0.51083\n",
      "SGD(171/199): loss=0.05031162740554289, weights=0.38721,0.1469,-0.54767\n",
      "SGD(172/199): loss=0.04807640129191116, weights=0.42485,0.12596,-0.56638\n",
      "SGD(173/199): loss=0.04574372494796, weights=0.44935,0.18172,-0.5327\n",
      "SGD(174/199): loss=0.04429934360419295, weights=0.4738,0.16052,-0.56125\n",
      "SGD(175/199): loss=0.04781599423541109, weights=0.42043,0.18259,-0.52837\n",
      "SGD(176/199): loss=0.049606642592863945, weights=0.39633,0.16933,-0.52766\n",
      "SGD(177/199): loss=0.047097416761696806, weights=0.42374,0.14354,-0.55656\n",
      "SGD(178/199): loss=0.04460701522367152, weights=0.46162,0.16125,-0.5414\n",
      "SGD(179/199): loss=0.04439009060974537, weights=0.47416,0.14724,-0.55437\n",
      "SGD(180/199): loss=0.04491021864587035, weights=0.48714,0.18545,-0.52695\n",
      "SGD(181/199): loss=0.04394564955252816, weights=0.50385,0.16987,-0.54309\n",
      "SGD(182/199): loss=0.04479883033146261, weights=0.54297,0.15874,-0.55212\n",
      "SGD(183/199): loss=0.044585137265045816, weights=0.48604,0.19024,-0.54081\n",
      "SGD(184/199): loss=0.0448186026317889, weights=0.47734,0.18746,-0.53575\n",
      "SGD(185/199): loss=0.04504356615731276, weights=0.46075,0.18581,-0.54316\n",
      "SGD(186/199): loss=0.046667812612755356, weights=0.47101,0.12671,-0.58376\n",
      "SGD(187/199): loss=0.049946259802731256, weights=0.4796,0.10786,-0.60494\n",
      "SGD(188/199): loss=0.05118724870484101, weights=0.47492,0.10586,-0.6131\n",
      "SGD(189/199): loss=0.049090625232321904, weights=0.47836,0.20981,-0.49431\n",
      "SGD(190/199): loss=0.04608765374548391, weights=0.502,0.19167,-0.51002\n",
      "SGD(191/199): loss=0.04471304530277686, weights=0.45883,0.16168,-0.54273\n",
      "SGD(192/199): loss=0.04429275796454176, weights=0.51396,0.17335,-0.53311\n",
      "SGD(193/199): loss=0.044784728795793274, weights=0.48864,0.18592,-0.53\n",
      "SGD(194/199): loss=0.04412194386225792, weights=0.52307,0.16506,-0.5497\n",
      "SGD(195/199): loss=0.045000669537365624, weights=0.5221,0.14457,-0.5743\n",
      "SGD(196/199): loss=0.04479712175431593, weights=0.53882,0.15438,-0.56071\n",
      "SGD(197/199): loss=0.044844409530197575, weights=0.51283,0.14453,-0.5744\n",
      "SGD(198/199): loss=0.045832595345812625, weights=0.50538,0.13607,-0.58636\n",
      "SGD(199/199): loss=0.04409839789570272, weights=0.48627,0.16618,-0.56923\n"
     ]
    }
   ],
   "source": [
    "# demo for least_squares_SGD\n",
    "# define the parameters\n",
    "batch_size = 10\n",
    "max_iters = 200\n",
    "gamma = 0.3\n",
    "# weight initialization\n",
    "n = len(tx.T)\n",
    "initial_w = np.random.randn(n)\n",
    "# run SGD\n",
    "losses, ws = least_squares_SGD(y, tx, initial_w, max_iters, gamma, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"least squares regression using normal equations\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = calculate_mse(y, tx, w)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5         0.15778576 -0.54427505]\n"
     ]
    }
   ],
   "source": [
    "# demo for least_squares\n",
    "_, weight = least_squares(y, tx)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# required functions\n",
    "def ridge_mse(y, tx, w):\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    mse = e.dot(e) / (2 * len(e))\n",
    "    return mse\n",
    "\n",
    "# main function\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"rige regression using normal equations\"\"\"\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = ridge_mse(y, tx, w)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion=0.5, degree=3, lambda=0.000, Training RMSE=0.240, Testing RMSE=0.312\n",
      "proportion=0.5, degree=3, lambda=0.000, Training RMSE=0.240, Testing RMSE=0.312\n",
      "proportion=0.5, degree=3, lambda=0.000, Training RMSE=0.240, Testing RMSE=0.312\n",
      "proportion=0.5, degree=3, lambda=0.000, Training RMSE=0.240, Testing RMSE=0.313\n",
      "proportion=0.5, degree=3, lambda=0.000, Training RMSE=0.240, Testing RMSE=0.316\n",
      "proportion=0.5, degree=3, lambda=0.001, Training RMSE=0.240, Testing RMSE=0.321\n",
      "proportion=0.5, degree=3, lambda=0.001, Training RMSE=0.241, Testing RMSE=0.330\n",
      "proportion=0.5, degree=3, lambda=0.003, Training RMSE=0.244, Testing RMSE=0.343\n",
      "proportion=0.5, degree=3, lambda=0.007, Training RMSE=0.250, Testing RMSE=0.353\n",
      "proportion=0.5, degree=3, lambda=0.016, Training RMSE=0.264, Testing RMSE=0.350\n",
      "proportion=0.5, degree=3, lambda=0.037, Training RMSE=0.297, Testing RMSE=0.345\n",
      "proportion=0.5, degree=3, lambda=0.085, Training RMSE=0.356, Testing RMSE=0.381\n",
      "proportion=0.5, degree=3, lambda=0.193, Training RMSE=0.427, Testing RMSE=0.464\n",
      "proportion=0.5, degree=3, lambda=0.439, Training RMSE=0.484, Testing RMSE=0.543\n",
      "proportion=0.5, degree=3, lambda=1.000, Training RMSE=0.518, Testing RMSE=0.593\n"
     ]
    }
   ],
   "source": [
    "# demo for ridge_regression\n",
    "path_dataset = \"exerciseData/dataEx3.csv\"\n",
    "data = np.loadtxt(path_dataset, delimiter=\",\", skiprows=1, unpack=True)\n",
    "x_reg = data[0]\n",
    "y_reg = data[1]\n",
    "\n",
    "seed = 56\n",
    "degree = 3\n",
    "ratio = 0.5\n",
    "# define parameter\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "# split data\n",
    "x_tr, x_te, y_tr, y_te = split_data(x_reg, y_reg, ratio, seed)\n",
    "# form tx\n",
    "tx_tr = build_poly(x_tr, degree)\n",
    "tx_te = build_poly(x_te, degree)\n",
    "\n",
    "# ridge regression with different lambda\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    # ridge regression\n",
    "    _, weight = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "    rmse_tr.append(np.sqrt(2 * ridge_mse(y_tr, tx_tr, weight)))\n",
    "    rmse_te.append(np.sqrt(2 * ridge_mse(y_te, tx_te, weight)))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "           p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# required function(s)\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_cost(y, tx, w):\n",
    "    \"\"\"cost for logistic regression \"\"\"\n",
    "    sig = sigmoid(tx.dot(w));\n",
    "    cost = (-y) * np.log(sig) - (1-y) * np.log(1-sig)\n",
    "    return np.mean(cost)\n",
    "\n",
    "def logistic_gradient(y, tx, w):\n",
    "    \"\"\"gradient for logistic regression \"\"\"\n",
    "    err = sigmoid(tx.dot(w)) - y\n",
    "    grad = tx.T.dot(err) / len(err)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main function(s)\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"logistic regression using GD\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient, loss\n",
    "        grad, _ = logistic_gradient(y, tx, w)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # calculate loss\n",
    "        loss = logistic_cost(y, tx, w)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD({bi}/{ti}): loss={l}, weights={},{},{}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, *w.round(5)))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD(0/199): loss=0.31816071486732994, weights=-0.41974,-1.30815,-1.147\n",
      "GD(1/199): loss=0.3161478102039085, weights=-0.4048,-1.31271,-1.16625\n",
      "GD(2/199): loss=0.3142678024546776, weights=-0.39055,-1.31689,-1.18503\n",
      "GD(3/199): loss=0.3125080695129788, weights=-0.37694,-1.32069,-1.20338\n",
      "GD(4/199): loss=0.3108574405135453, weights=-0.36394,-1.32416,-1.22132\n",
      "GD(5/199): loss=0.30930600081444326, weights=-0.3515,-1.3273,-1.23888\n",
      "GD(6/199): loss=0.3078449269687315, weights=-0.33961,-1.33014,-1.25606\n",
      "GD(7/199): loss=0.3064663465026151, weights=-0.32823,-1.33269,-1.2729\n",
      "GD(8/199): loss=0.305163218307785, weights=-0.31734,-1.33497,-1.2894\n",
      "GD(9/199): loss=0.3039292302398596, weights=-0.3069,-1.337,-1.3056\n",
      "GD(10/199): loss=0.3027587111388808, weights=-0.29691,-1.33879,-1.32149\n",
      "GD(11/199): loss=0.3016465549869755, weights=-0.28732,-1.34035,-1.33709\n",
      "GD(12/199): loss=0.3005881553195915, weights=-0.27813,-1.34169,-1.35243\n",
      "GD(13/199): loss=0.29957934833087946, weights=-0.26932,-1.34283,-1.3675\n",
      "GD(14/199): loss=0.29861636337688297, weights=-0.26086,-1.34377,-1.38233\n",
      "GD(15/199): loss=0.2976957797946601, weights=-0.25274,-1.34452,-1.39691\n",
      "GD(16/199): loss=0.29681448913104286, weights=-0.24495,-1.3451,-1.41127\n",
      "GD(17/199): loss=0.29596966201907887, weights=-0.23746,-1.3455,-1.42541\n",
      "GD(18/199): loss=0.29515871905932745, weights=-0.23026,-1.34575,-1.43933\n",
      "GD(19/199): loss=0.2943793051618752, weights=-0.22335,-1.34584,-1.45305\n",
      "GD(20/199): loss=0.2936292668870043, weights=-0.2167,-1.34578,-1.46658\n",
      "GD(21/199): loss=0.29290663239092657, weights=-0.21031,-1.34558,-1.47992\n",
      "GD(22/199): loss=0.29220959364033405, weights=-0.20417,-1.34524,-1.49307\n",
      "GD(23/199): loss=0.29153649060768816, weights=-0.19826,-1.34478,-1.50605\n",
      "GD(24/199): loss=0.2908857971997552, weights=-0.19257,-1.34419,-1.51887\n",
      "GD(25/199): loss=0.2902561087062091, weights=-0.18709,-1.34348,-1.53151\n",
      "GD(26/199): loss=0.2896461305842078, weights=-0.18182,-1.34266,-1.544\n",
      "GD(27/199): loss=0.28905466841957644, weights=-0.17675,-1.34172,-1.55634\n",
      "GD(28/199): loss=0.28848061892631244, weights=-0.17186,-1.34068,-1.56852\n",
      "GD(29/199): loss=0.28792296186414257, weights=-0.16715,-1.33954,-1.58057\n",
      "GD(30/199): loss=0.28738075276930486, weights=-0.16262,-1.3383,-1.59247\n",
      "GD(31/199): loss=0.28685311640698885, weights=-0.15825,-1.33696,-1.60424\n",
      "GD(32/199): loss=0.28633924086529033, weights=-0.15403,-1.33553,-1.61587\n",
      "GD(33/199): loss=0.28583837222039166, weights=-0.14997,-1.33402,-1.62738\n",
      "GD(34/199): loss=0.2853498097112072, weights=-0.14606,-1.33242,-1.63876\n",
      "GD(35/199): loss=0.28487290136912147, weights=-0.14229,-1.33074,-1.65003\n",
      "GD(36/199): loss=0.2844070400548714, weights=-0.13865,-1.32898,-1.66117\n",
      "GD(37/199): loss=0.2839516598602082, weights=-0.13514,-1.32714,-1.6722\n",
      "GD(38/199): loss=0.28350623283685433, weights=-0.13175,-1.32523,-1.68312\n",
      "GD(39/199): loss=0.28307026601952323, weights=-0.12848,-1.32325,-1.69393\n",
      "GD(40/199): loss=0.28264329871350297, weights=-0.12533,-1.32119,-1.70464\n",
      "GD(41/199): loss=0.28222490002057365, weights=-0.12229,-1.31908,-1.71524\n",
      "GD(42/199): loss=0.2818146665798984, weights=-0.11935,-1.31689,-1.72574\n",
      "GD(43/199): loss=0.28141222050305853, weights=-0.11652,-1.31465,-1.73614\n",
      "GD(44/199): loss=0.28101720748463066, weights=-0.11378,-1.31235,-1.74645\n",
      "GD(45/199): loss=0.28062929507167017, weights=-0.11114,-1.30998,-1.75667\n",
      "GD(46/199): loss=0.28024817107720407, weights=-0.10859,-1.30756,-1.76679\n",
      "GD(47/199): loss=0.279873542124379, weights=-0.10612,-1.30509,-1.77682\n",
      "GD(48/199): loss=0.2795051323092732, weights=-0.10374,-1.30256,-1.78677\n",
      "GD(49/199): loss=0.2791426819715957, weights=-0.10144,-1.29998,-1.79663\n",
      "GD(50/199): loss=0.27878594656357414, weights=-0.09922,-1.29736,-1.80641\n",
      "GD(51/199): loss=0.27843469560829515, weights=-0.09708,-1.29468,-1.8161\n",
      "GD(52/199): loss=0.278088711739614, weights=-0.095,-1.29196,-1.82572\n",
      "GD(53/199): loss=0.2777477898165198, weights=-0.093,-1.28919,-1.83526\n",
      "GD(54/199): loss=0.27741173610552156, weights=-0.09106,-1.28638,-1.84472\n",
      "GD(55/199): loss=0.2770803675252356, weights=-0.08919,-1.28352,-1.8541\n",
      "GD(56/199): loss=0.27675351094790296, weights=-0.08738,-1.28063,-1.86342\n",
      "GD(57/199): loss=0.2764310025530541, weights=-0.08563,-1.27769,-1.87266\n",
      "GD(58/199): loss=0.2761126872289842, weights=-0.08394,-1.27471,-1.88183\n",
      "GD(59/199): loss=0.2757984180180971, weights=-0.08231,-1.2717,-1.89093\n",
      "GD(60/199): loss=0.27548805560253375, weights=-0.08073,-1.26865,-1.89996\n",
      "GD(61/199): loss=0.2751814678268231, weights=-0.0792,-1.26557,-1.90893\n",
      "GD(62/199): loss=0.2748785292545859, weights=-0.07772,-1.26245,-1.91783\n",
      "GD(63/199): loss=0.2745791207565794, weights=-0.07629,-1.25929,-1.92667\n",
      "GD(64/199): loss=0.2742831291276151, weights=-0.07491,-1.25611,-1.93544\n",
      "GD(65/199): loss=0.2739904467300875, weights=-0.07357,-1.25289,-1.94416\n",
      "GD(66/199): loss=0.2737009711620547, weights=-0.07227,-1.24964,-1.95281\n",
      "GD(67/199): loss=0.27341460494798103, weights=-0.07102,-1.24637,-1.9614\n",
      "GD(68/199): loss=0.27313125525041415, weights=-0.06981,-1.24306,-1.96993\n",
      "GD(69/199): loss=0.27285083360101464, weights=-0.06864,-1.23973,-1.97841\n",
      "GD(70/199): loss=0.27257325564948626, weights=-0.06751,-1.23637,-1.98683\n",
      "GD(71/199): loss=0.27229844092907485, weights=-0.06641,-1.23298,-1.9952\n",
      "GD(72/199): loss=0.27202631263741456, weights=-0.06535,-1.22957,-2.00351\n",
      "GD(73/199): loss=0.2717567974315963, weights=-0.06432,-1.22613,-2.01176\n",
      "GD(74/199): loss=0.2714898252364276, weights=-0.06333,-1.22267,-2.01997\n",
      "GD(75/199): loss=0.2712253290649314, weights=-0.06236,-1.21918,-2.02812\n",
      "GD(76/199): loss=0.27096324485021067, weights=-0.06143,-1.21567,-2.03622\n",
      "GD(77/199): loss=0.2707035112878712, weights=-0.06053,-1.21214,-2.04427\n",
      "GD(78/199): loss=0.27044606968826035, weights=-0.05966,-1.20859,-2.05227\n",
      "GD(79/199): loss=0.2701908638378358, weights=-0.05881,-1.20502,-2.06023\n",
      "GD(80/199): loss=0.26993783986903125, weights=-0.05799,-1.20143,-2.06813\n",
      "GD(81/199): loss=0.26968694613803523, weights=-0.0572,-1.19781,-2.07599\n",
      "GD(82/199): loss=0.2694381331099409, weights=-0.05643,-1.19418,-2.0838\n",
      "GD(83/199): loss=0.26919135325077065, weights=-0.05569,-1.19054,-2.09157\n",
      "GD(84/199): loss=0.26894656092590885, weights=-0.05497,-1.18687,-2.09929\n",
      "GD(85/199): loss=0.26870371230451895, weights=-0.05428,-1.18318,-2.10697\n",
      "GD(86/199): loss=0.26846276526954516, weights=-0.0536,-1.17948,-2.1146\n",
      "GD(87/199): loss=0.26822367933293323, weights=-0.05295,-1.17577,-2.12219\n",
      "GD(88/199): loss=0.26798641555572833, weights=-0.05232,-1.17203,-2.12974\n",
      "GD(89/199): loss=0.2677509364727346, weights=-0.05171,-1.16829,-2.13725\n",
      "GD(90/199): loss=0.2675172060214424, weights=-0.05111,-1.16452,-2.14471\n",
      "GD(91/199): loss=0.2672851894749503, weights=-0.05054,-1.16075,-2.15214\n",
      "GD(92/199): loss=0.2670548533786277, weights=-0.04998,-1.15696,-2.15953\n",
      "GD(93/199): loss=0.2668261654902856, weights=-0.04944,-1.15315,-2.16687\n",
      "GD(94/199): loss=0.26659909472363025, weights=-0.04892,-1.14934,-2.17418\n",
      "GD(95/199): loss=0.266373611094803, weights=-0.04841,-1.14551,-2.18145\n",
      "GD(96/199): loss=0.26614968567180847, weights=-0.04792,-1.14167,-2.18868\n",
      "GD(97/199): loss=0.26592729052666125, weights=-0.04745,-1.13781,-2.19587\n",
      "GD(98/199): loss=0.2657063986900801, weights=-0.04699,-1.13395,-2.20303\n",
      "GD(99/199): loss=0.2654869841085794, weights=-0.04654,-1.13007,-2.21015\n",
      "GD(100/199): loss=0.26526902160381244, weights=-0.04611,-1.12619,-2.21724\n",
      "GD(101/199): loss=0.26505248683403326, weights=-0.04569,-1.12229,-2.22429\n",
      "GD(102/199): loss=0.2648373562575507, weights=-0.04529,-1.11839,-2.23131\n",
      "GD(103/199): loss=0.2646236070980592, weights=-0.0449,-1.11447,-2.23829\n",
      "GD(104/199): loss=0.2644112173117354, weights=-0.04452,-1.11055,-2.24524\n",
      "GD(105/199): loss=0.26420016555599896, weights=-0.04415,-1.10661,-2.25215\n",
      "GD(106/199): loss=0.2639904311598423, weights=-0.04379,-1.10267,-2.25903\n",
      "GD(107/199): loss=0.263781994095638, weights=-0.04344,-1.09872,-2.26588\n",
      "GD(108/199): loss=0.2635748349523424, weights=-0.04311,-1.09476,-2.2727\n",
      "GD(109/199): loss=0.2633689349100144, weights=-0.04278,-1.0908,-2.27948\n",
      "GD(110/199): loss=0.26316427571557727, weights=-0.04247,-1.08683,-2.28624\n",
      "GD(111/199): loss=0.26296083965975353, weights=-0.04216,-1.08285,-2.29296\n",
      "GD(112/199): loss=0.26275860955510916, weights=-0.04187,-1.07886,-2.29965\n",
      "GD(113/199): loss=0.26255756871514524, weights=-0.04158,-1.07487,-2.30631\n",
      "GD(114/199): loss=0.26235770093437955, weights=-0.0413,-1.07087,-2.31295\n",
      "GD(115/199): loss=0.2621589904693671, weights=-0.04103,-1.06687,-2.31955\n",
      "GD(116/199): loss=0.26196142202060535, weights=-0.04077,-1.06286,-2.32612\n",
      "GD(117/199): loss=0.2617649807152806, weights=-0.04052,-1.05885,-2.33267\n",
      "GD(118/199): loss=0.26156965209080774, weights=-0.04027,-1.05483,-2.33918\n",
      "GD(119/199): loss=0.26137542207912384, weights=-0.04003,-1.0508,-2.34567\n",
      "GD(120/199): loss=0.26118227699169516, weights=-0.0398,-1.04677,-2.35213\n",
      "GD(121/199): loss=0.2609902035052003, weights=-0.03958,-1.04274,-2.35857\n",
      "GD(122/199): loss=0.260799188647855, weights=-0.03936,-1.0387,-2.36497\n",
      "GD(123/199): loss=0.2606092197863458, weights=-0.03915,-1.03466,-2.37135\n",
      "GD(124/199): loss=0.26042028461334055, weights=-0.03895,-1.03062,-2.3777\n",
      "GD(125/199): loss=0.26023237113554787, weights=-0.03875,-1.02657,-2.38403\n",
      "GD(126/199): loss=0.26004546766229697, weights=-0.03856,-1.02252,-2.39033\n",
      "GD(127/199): loss=0.25985956279461225, weights=-0.03837,-1.01846,-2.39661\n",
      "GD(128/199): loss=0.2596746454147579, weights=-0.03819,-1.01441,-2.40286\n",
      "GD(129/199): loss=0.25949070467622953, weights=-0.03802,-1.01035,-2.40908\n",
      "GD(130/199): loss=0.2593077299941705, weights=-0.03785,-1.00628,-2.41528\n",
      "GD(131/199): loss=0.2591257110361932, weights=-0.03768,-1.00222,-2.42146\n",
      "GD(132/199): loss=0.2589446377135838, weights=-0.03752,-0.99815,-2.42761\n",
      "GD(133/199): loss=0.2587645001728748, weights=-0.03737,-0.99409,-2.43373\n",
      "GD(134/199): loss=0.2585852887877649, weights=-0.03722,-0.99002,-2.43984\n",
      "GD(135/199): loss=0.25840699415137197, weights=-0.03707,-0.98595,-2.44592\n",
      "GD(136/199): loss=0.2582296070688018, weights=-0.03693,-0.98187,-2.45197\n",
      "GD(137/199): loss=0.25805311855001944, weights=-0.03679,-0.9778,-2.45801\n",
      "GD(138/199): loss=0.2578775198030064, weights=-0.03666,-0.97373,-2.46402\n",
      "GD(139/199): loss=0.25770280222719427, weights=-0.03653,-0.96965,-2.47\n",
      "GD(140/199): loss=0.2575289574071589, weights=-0.03641,-0.96557,-2.47597\n",
      "GD(141/199): loss=0.25735597710656355, weights=-0.03628,-0.9615,-2.48191\n",
      "GD(142/199): loss=0.2571838532623419, weights=-0.03617,-0.95742,-2.48784\n",
      "GD(143/199): loss=0.2570125779791077, weights=-0.03605,-0.95335,-2.49374\n",
      "GD(144/199): loss=0.25684214352378193, weights=-0.03594,-0.94927,-2.49962\n",
      "GD(145/199): loss=0.25667254232042785, weights=-0.03583,-0.94519,-2.50547\n",
      "GD(146/199): loss=0.2565037669452843, weights=-0.03573,-0.94112,-2.51131\n",
      "GD(147/199): loss=0.25633581012198864, weights=-0.03563,-0.93704,-2.51713\n",
      "GD(148/199): loss=0.2561686647169815, weights=-0.03553,-0.93296,-2.52292\n",
      "GD(149/199): loss=0.2560023237350846, weights=-0.03543,-0.92889,-2.5287\n",
      "GD(150/199): loss=0.2558367803152449, weights=-0.03534,-0.92482,-2.53445\n",
      "GD(151/199): loss=0.25567202772643743, weights=-0.03525,-0.92074,-2.54018\n",
      "GD(152/199): loss=0.25550805936372006, weights=-0.03516,-0.91667,-2.5459\n",
      "GD(153/199): loss=0.25534486874443435, weights=-0.03507,-0.9126,-2.5516\n",
      "GD(154/199): loss=0.255182449504545, weights=-0.03499,-0.90853,-2.55727\n",
      "GD(155/199): loss=0.2550207953951142, weights=-0.03491,-0.90446,-2.56293\n",
      "GD(156/199): loss=0.2548599002789023, weights=-0.03483,-0.9004,-2.56857\n",
      "GD(157/199): loss=0.25469975812709295, weights=-0.03476,-0.89633,-2.57418\n",
      "GD(158/199): loss=0.2545403630161341, weights=-0.03468,-0.89227,-2.57978\n",
      "GD(159/199): loss=0.2543817091246934, weights=-0.03461,-0.88821,-2.58537\n",
      "GD(160/199): loss=0.25422379073072054, weights=-0.03454,-0.88415,-2.59093\n",
      "GD(161/199): loss=0.25406660220861443, weights=-0.03448,-0.88009,-2.59647\n",
      "GD(162/199): loss=0.2539101380264887, weights=-0.03441,-0.87603,-2.602\n",
      "GD(163/199): loss=0.2537543927435342, weights=-0.03435,-0.87198,-2.60751\n",
      "GD(164/199): loss=0.25359936100747216, weights=-0.03428,-0.86793,-2.613\n",
      "GD(165/199): loss=0.2534450375520965, weights=-0.03422,-0.86388,-2.61847\n",
      "GD(166/199): loss=0.25329141719489956, weights=-0.03416,-0.85984,-2.62393\n",
      "GD(167/199): loss=0.25313849483478035, weights=-0.03411,-0.8558,-2.62937\n",
      "GD(168/199): loss=0.2529862654498302, weights=-0.03405,-0.85176,-2.63479\n",
      "GD(169/199): loss=0.252834724095194, weights=-0.034,-0.84772,-2.6402\n",
      "GD(170/199): loss=0.252683865901003, weights=-0.03394,-0.84368,-2.64558\n",
      "GD(171/199): loss=0.2525336860703778, weights=-0.03389,-0.83965,-2.65096\n",
      "GD(172/199): loss=0.2523841798774978, weights=-0.03384,-0.83562,-2.65631\n",
      "GD(173/199): loss=0.25223534266573455, weights=-0.03379,-0.8316,-2.66165\n",
      "GD(174/199): loss=0.2520871698458475, weights=-0.03375,-0.82758,-2.66697\n",
      "GD(175/199): loss=0.2519396568942387, weights=-0.0337,-0.82356,-2.67228\n",
      "GD(176/199): loss=0.25179279935126486, weights=-0.03366,-0.81955,-2.67757\n",
      "GD(177/199): loss=0.251646592819605, weights=-0.03361,-0.81553,-2.68284\n",
      "GD(178/199): loss=0.2515010329626803, weights=-0.03357,-0.81153,-2.6881\n",
      "GD(179/199): loss=0.25135611550312587, weights=-0.03353,-0.80752,-2.69334\n",
      "GD(180/199): loss=0.25121183622131116, weights=-0.03349,-0.80352,-2.69857\n",
      "GD(181/199): loss=0.2510681909539081, weights=-0.03345,-0.79952,-2.70378\n",
      "GD(182/199): loss=0.25092517559250443, weights=-0.03341,-0.79553,-2.70898\n",
      "GD(183/199): loss=0.25078278608226234, weights=-0.03337,-0.79154,-2.71416\n",
      "GD(184/199): loss=0.25064101842061765, weights=-0.03333,-0.78756,-2.71933\n",
      "GD(185/199): loss=0.2504998686560217, weights=-0.0333,-0.78358,-2.72448\n",
      "GD(186/199): loss=0.2503593328867213, weights=-0.03326,-0.7796,-2.72962\n",
      "GD(187/199): loss=0.2502194072595781, weights=-0.03323,-0.77563,-2.73474\n",
      "GD(188/199): loss=0.2500800879689231, weights=-0.0332,-0.77166,-2.73985\n",
      "GD(189/199): loss=0.24994137125544785, weights=-0.03316,-0.76769,-2.74494\n",
      "GD(190/199): loss=0.2498032534051295, weights=-0.03313,-0.76373,-2.75002\n",
      "GD(191/199): loss=0.24966573074818826, weights=-0.0331,-0.75978,-2.75508\n",
      "GD(192/199): loss=0.24952879965807725, weights=-0.03307,-0.75583,-2.76014\n",
      "GD(193/199): loss=0.24939245655050296, weights=-0.03304,-0.75188,-2.76517\n",
      "GD(194/199): loss=0.24925669788247545, weights=-0.03301,-0.74794,-2.7702\n",
      "GD(195/199): loss=0.24912152015138672, weights=-0.03298,-0.744,-2.7752\n",
      "GD(196/199): loss=0.24898691989411736, weights=-0.03296,-0.74007,-2.7802\n",
      "GD(197/199): loss=0.24885289368616945, weights=-0.03293,-0.73614,-2.78518\n",
      "GD(198/199): loss=0.2487194381408257, weights=-0.0329,-0.73222,-2.79015\n",
      "GD(199/199): loss=0.24858654990833287, weights=-0.03288,-0.7283,-2.79511\n"
     ]
    }
   ],
   "source": [
    "# demo for logistic_regression\n",
    "# define the parameters\n",
    "max_iters = 200\n",
    "gamma = 0.3\n",
    "# weight initialization\n",
    "n = num_features = len(tx.T)\n",
    "initial_w = np.random.randn(n)\n",
    "# run GD\n",
    "losses, ws = logistic_regression(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_SGD(y, tx, initial_w, max_iters, gamma, batch_size=1):\n",
    "    \"\"\"linear regression using stochastic SGD\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = logistic_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = logistic_cost(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, weights={},{},{}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, *w.round(5)))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/199): loss=1.675245019115256, weights=0.00123,0.38271,1.36631\n",
      "SGD(1/199): loss=1.4221540943245519, weights=-0.05173,0.1974,1.16086\n",
      "SGD(2/199): loss=1.231697335641256, weights=-0.02329,0.05679,0.98964\n",
      "SGD(3/199): loss=1.0438159578013746, weights=-0.10144,-0.09203,0.80228\n",
      "SGD(4/199): loss=0.8807310815491292, weights=-0.08748,-0.23648,0.62113\n",
      "SGD(5/199): loss=0.752133456475574, weights=-0.04521,-0.3728,0.46033\n",
      "SGD(6/199): loss=0.6505464946313978, weights=0.01835,-0.5042,0.31135\n",
      "SGD(7/199): loss=0.5658443369293982, weights=-0.02976,-0.65041,0.16823\n",
      "SGD(8/199): loss=0.5190389618856986, weights=-0.04798,-0.73918,0.06499\n",
      "SGD(9/199): loss=0.485476602563933, weights=-0.06044,-0.8067,-0.02539\n",
      "SGD(10/199): loss=0.46114303434313386, weights=0.01793,-0.86931,-0.09261\n",
      "SGD(11/199): loss=0.43884382287873125, weights=0.04205,-0.92706,-0.16891\n",
      "SGD(12/199): loss=0.42709715790141983, weights=0.06093,-0.94447,-0.22413\n",
      "SGD(13/199): loss=0.41265193416737045, weights=0.08251,-0.98605,-0.28605\n",
      "SGD(14/199): loss=0.40039340829850645, weights=0.0585,-1.02621,-0.33868\n",
      "SGD(15/199): loss=0.39550485159157855, weights=0.03104,-1.03259,-0.36645\n",
      "SGD(16/199): loss=0.38995867695565045, weights=0.0373,-1.03103,-0.40659\n",
      "SGD(17/199): loss=0.3824367752088544, weights=0.01273,-1.05628,-0.44628\n",
      "SGD(18/199): loss=0.37116616835613925, weights=-0.01748,-1.10583,-0.50762\n",
      "SGD(19/199): loss=0.3671256616745971, weights=-0.07842,-1.10649,-0.54423\n",
      "SGD(20/199): loss=0.365496505614659, weights=-0.10845,-1.10168,-0.56411\n",
      "SGD(21/199): loss=0.3572055360901542, weights=-0.07472,-1.13426,-0.61737\n",
      "SGD(22/199): loss=0.35043738426525445, weights=-0.04372,-1.1671,-0.66387\n",
      "SGD(23/199): loss=0.34647315456555977, weights=-0.01867,-1.17503,-0.69914\n",
      "SGD(24/199): loss=0.34196845150329463, weights=-0.01567,-1.207,-0.73241\n",
      "SGD(25/199): loss=0.33819336687051127, weights=-0.07068,-1.23337,-0.76579\n",
      "SGD(26/199): loss=0.3353460580920585, weights=-0.08317,-1.25131,-0.79268\n",
      "SGD(27/199): loss=0.3312505944127783, weights=-0.05937,-1.28122,-0.82843\n",
      "SGD(28/199): loss=0.3273785427579036, weights=-0.02789,-1.30998,-0.86609\n",
      "SGD(29/199): loss=0.32829158259049507, weights=-0.06601,-1.29286,-0.862\n",
      "SGD(30/199): loss=0.32652193288780934, weights=-0.10512,-1.28987,-0.89055\n",
      "SGD(31/199): loss=0.3237577471014761, weights=-0.08695,-1.2951,-0.92281\n",
      "SGD(32/199): loss=0.3252898965902121, weights=-0.1606,-1.26676,-0.92561\n",
      "SGD(33/199): loss=0.3230956553830291, weights=-0.17764,-1.28902,-0.9527\n",
      "SGD(34/199): loss=0.32058652520546455, weights=-0.10984,-1.28707,-0.97198\n",
      "SGD(35/199): loss=0.31900748636876597, weights=-0.06372,-1.29542,-0.98584\n",
      "SGD(36/199): loss=0.31741005090753427, weights=-0.06556,-1.29726,-1.00848\n",
      "SGD(37/199): loss=0.31708329618617437, weights=-0.11803,-1.27075,-1.02778\n",
      "SGD(38/199): loss=0.31687768651383696, weights=-0.15263,-1.26563,-1.03992\n",
      "SGD(39/199): loss=0.3179940713342816, weights=-0.14918,-1.22667,-1.03426\n",
      "SGD(40/199): loss=0.31536577380657216, weights=-0.12521,-1.24535,-1.06116\n",
      "SGD(41/199): loss=0.3134643801084822, weights=-0.14789,-1.27052,-1.08817\n",
      "SGD(42/199): loss=0.3108774555677578, weights=-0.11564,-1.28785,-1.11704\n",
      "SGD(43/199): loss=0.31227020293148433, weights=-0.17229,-1.2649,-1.11469\n",
      "SGD(44/199): loss=0.30987429946034356, weights=-0.1379,-1.27924,-1.13979\n",
      "SGD(45/199): loss=0.3074736954977948, weights=-0.10216,-1.30086,-1.16743\n",
      "SGD(46/199): loss=0.3058279383983487, weights=-0.09523,-1.32418,-1.19039\n",
      "SGD(47/199): loss=0.3040294591346758, weights=-0.06304,-1.35031,-1.21462\n",
      "SGD(48/199): loss=0.3030159284568385, weights=-0.05458,-1.35621,-1.23184\n",
      "SGD(49/199): loss=0.3020686701608853, weights=-0.05282,-1.36935,-1.24818\n",
      "SGD(50/199): loss=0.3012835824954381, weights=-0.05984,-1.38492,-1.26258\n",
      "SGD(51/199): loss=0.30026249336372807, weights=-0.06764,-1.40511,-1.28236\n",
      "SGD(52/199): loss=0.2992585566496763, weights=-0.11923,-1.41341,-1.31103\n",
      "SGD(53/199): loss=0.2980857497100957, weights=-0.08724,-1.41458,-1.32942\n",
      "SGD(54/199): loss=0.29697028496323785, weights=-0.0792,-1.42703,-1.35225\n",
      "SGD(55/199): loss=0.2960324133584165, weights=-0.0525,-1.44101,-1.37068\n",
      "SGD(56/199): loss=0.2953373953349148, weights=-0.07182,-1.44826,-1.38862\n",
      "SGD(57/199): loss=0.29469984523220794, weights=-0.07993,-1.45558,-1.40493\n",
      "SGD(58/199): loss=0.2942280887957876, weights=-0.08877,-1.47297,-1.41909\n",
      "SGD(59/199): loss=0.29397867239723735, weights=-0.08486,-1.48139,-1.42544\n",
      "SGD(60/199): loss=0.29383518899768574, weights=-0.07155,-1.47997,-1.42692\n",
      "SGD(61/199): loss=0.29346684718263905, weights=-0.06787,-1.48976,-1.4368\n",
      "SGD(62/199): loss=0.29263495316330973, weights=-0.04458,-1.49957,-1.45724\n",
      "SGD(63/199): loss=0.2917260921410105, weights=-0.0547,-1.50555,-1.48229\n",
      "SGD(64/199): loss=0.2912744907654187, weights=0.00116,-1.4847,-1.49007\n",
      "SGD(65/199): loss=0.2910322137294755, weights=-0.03211,-1.46496,-1.4918\n",
      "SGD(66/199): loss=0.29014136038840616, weights=-0.02125,-1.47129,-1.51606\n",
      "SGD(67/199): loss=0.28935338045964865, weights=-0.02921,-1.44208,-1.53125\n",
      "SGD(68/199): loss=0.28881737973755256, weights=-0.01164,-1.43036,-1.54352\n",
      "SGD(69/199): loss=0.28817288015636944, weights=0.00123,-1.43804,-1.56292\n",
      "SGD(70/199): loss=0.2876145668478027, weights=0.00641,-1.40087,-1.57126\n",
      "SGD(71/199): loss=0.2867797143634148, weights=-0.01095,-1.40866,-1.59392\n",
      "SGD(72/199): loss=0.2868194013097521, weights=0.02732,-1.38811,-1.59273\n",
      "SGD(73/199): loss=0.2871125728716001, weights=0.03011,-1.39862,-1.58723\n",
      "SGD(74/199): loss=0.2864237189900703, weights=0.01195,-1.41087,-1.60595\n",
      "SGD(75/199): loss=0.2858024648803305, weights=-0.02695,-1.41758,-1.62253\n",
      "SGD(76/199): loss=0.2855244234204561, weights=-0.02243,-1.42752,-1.63292\n",
      "SGD(77/199): loss=0.2850345864798478, weights=-0.03335,-1.39908,-1.6399\n",
      "SGD(78/199): loss=0.2842682307186865, weights=-0.03937,-1.39467,-1.66085\n",
      "SGD(79/199): loss=0.2840276467368045, weights=-0.05884,-1.40121,-1.6707\n",
      "SGD(80/199): loss=0.2836971841767354, weights=-0.06424,-1.40844,-1.68293\n",
      "SGD(81/199): loss=0.28318069454884454, weights=-0.07387,-1.40839,-1.69953\n",
      "SGD(82/199): loss=0.28267010341954923, weights=-0.03071,-1.41352,-1.71356\n",
      "SGD(83/199): loss=0.2823534042595646, weights=-0.05297,-1.38872,-1.71648\n",
      "SGD(84/199): loss=0.28173336445665353, weights=-0.05574,-1.39068,-1.73628\n",
      "SGD(85/199): loss=0.281239253650655, weights=-0.04123,-1.4063,-1.75621\n",
      "SGD(86/199): loss=0.28130822857687754, weights=-0.05198,-1.43128,-1.76398\n",
      "SGD(87/199): loss=0.2810188535357145, weights=-0.04778,-1.44463,-1.77884\n",
      "SGD(88/199): loss=0.2813709859746335, weights=-0.10232,-1.45778,-1.78068\n",
      "SGD(89/199): loss=0.2811776208160525, weights=-0.10109,-1.44028,-1.77936\n",
      "SGD(90/199): loss=0.2810008846080131, weights=-0.12069,-1.4534,-1.79623\n",
      "SGD(91/199): loss=0.2806134396188311, weights=-0.09537,-1.45385,-1.80325\n",
      "SGD(92/199): loss=0.2803787988449339, weights=-0.11964,-1.45521,-1.81831\n",
      "SGD(93/199): loss=0.2798934259133681, weights=-0.1041,-1.46751,-1.83753\n",
      "SGD(94/199): loss=0.2791599785491585, weights=-0.09181,-1.447,-1.85059\n",
      "SGD(95/199): loss=0.2790168058598712, weights=-0.11312,-1.45294,-1.8642\n",
      "SGD(96/199): loss=0.2790629198853586, weights=-0.12624,-1.40796,-1.84436\n",
      "SGD(97/199): loss=0.2793609084057106, weights=-0.13507,-1.39607,-1.8319\n",
      "SGD(98/199): loss=0.27898777879182507, weights=-0.11798,-1.41177,-1.84617\n",
      "SGD(99/199): loss=0.27881655478428075, weights=-0.12577,-1.40842,-1.85303\n",
      "SGD(100/199): loss=0.27952009617125206, weights=-0.171,-1.39115,-1.83904\n",
      "SGD(101/199): loss=0.27939872917870334, weights=-0.17301,-1.40441,-1.84979\n",
      "SGD(102/199): loss=0.27797447243058526, weights=-0.14907,-1.35644,-1.86808\n",
      "SGD(103/199): loss=0.27785347635785995, weights=-0.15649,-1.37116,-1.88155\n",
      "SGD(104/199): loss=0.2770098417654247, weights=-0.12841,-1.36656,-1.89863\n",
      "SGD(105/199): loss=0.2770289257521836, weights=-0.15881,-1.36955,-1.91118\n",
      "SGD(106/199): loss=0.27679079557454866, weights=-0.16543,-1.37336,-1.92471\n",
      "SGD(107/199): loss=0.27657369791583425, weights=-0.16387,-1.38234,-1.9366\n",
      "SGD(108/199): loss=0.27607374148040364, weights=-0.14996,-1.38739,-1.95203\n",
      "SGD(109/199): loss=0.2758788899622127, weights=-0.16571,-1.36429,-1.95416\n",
      "SGD(110/199): loss=0.27559600519278227, weights=-0.14907,-1.37901,-1.96538\n",
      "SGD(111/199): loss=0.2749995992001943, weights=-0.11752,-1.38448,-1.98017\n",
      "SGD(112/199): loss=0.27489551401354345, weights=-0.11086,-1.3825,-1.9811\n",
      "SGD(113/199): loss=0.2747181639565626, weights=-0.0941,-1.38994,-1.98835\n",
      "SGD(114/199): loss=0.27447284693192603, weights=-0.07921,-1.39988,-2.0017\n",
      "SGD(115/199): loss=0.27428526588683677, weights=-0.06593,-1.4051,-2.01101\n",
      "SGD(116/199): loss=0.27458400374328057, weights=-0.1127,-1.41156,-2.01305\n",
      "SGD(117/199): loss=0.27431938069676526, weights=-0.11729,-1.42231,-2.03349\n",
      "SGD(118/199): loss=0.27427938343768976, weights=-0.13313,-1.42973,-2.04658\n",
      "SGD(119/199): loss=0.27466831669120145, weights=-0.1815,-1.40949,-2.03842\n",
      "SGD(120/199): loss=0.2740032063864366, weights=-0.16208,-1.39048,-2.04301\n",
      "SGD(121/199): loss=0.2741335428127493, weights=-0.1721,-1.39402,-2.04514\n",
      "SGD(122/199): loss=0.27415579522297695, weights=-0.18272,-1.40794,-2.06014\n",
      "SGD(123/199): loss=0.27351535924752035, weights=-0.20231,-1.33699,-2.05182\n",
      "SGD(124/199): loss=0.2737217798410084, weights=-0.22401,-1.34364,-2.06229\n",
      "SGD(125/199): loss=0.27366224519868576, weights=-0.2369,-1.326,-2.06371\n",
      "SGD(126/199): loss=0.2727208945686548, weights=-0.20813,-1.31363,-2.07407\n",
      "SGD(127/199): loss=0.2721726791655651, weights=-0.19555,-1.31243,-2.08784\n",
      "SGD(128/199): loss=0.27222526645229234, weights=-0.21424,-1.32603,-2.10685\n",
      "SGD(129/199): loss=0.27207960026142647, weights=-0.21173,-1.33277,-2.11593\n",
      "SGD(130/199): loss=0.27182974132872906, weights=-0.21559,-1.33624,-2.13215\n",
      "SGD(131/199): loss=0.27155418430469525, weights=-0.19315,-1.33522,-2.12821\n",
      "SGD(132/199): loss=0.27118688895661336, weights=-0.17759,-1.34179,-2.13998\n",
      "SGD(133/199): loss=0.271802675751682, weights=-0.21495,-1.34235,-2.13736\n",
      "SGD(134/199): loss=0.27077821794925605, weights=-0.16269,-1.33589,-2.14577\n",
      "SGD(135/199): loss=0.2704663180013285, weights=-0.18201,-1.32597,-2.16357\n",
      "SGD(136/199): loss=0.2703268980316936, weights=-0.18312,-1.33271,-2.17605\n",
      "SGD(137/199): loss=0.2699141631993735, weights=-0.17365,-1.31942,-2.1791\n",
      "SGD(138/199): loss=0.26923056008315244, weights=-0.14477,-1.3077,-2.18667\n",
      "SGD(139/199): loss=0.2689364508878944, weights=-0.12672,-1.31736,-2.20076\n",
      "SGD(140/199): loss=0.2688730586422103, weights=-0.13172,-1.32868,-2.21572\n",
      "SGD(141/199): loss=0.26868649234863234, weights=-0.13912,-1.32772,-2.22749\n",
      "SGD(142/199): loss=0.2679347900066022, weights=-0.09428,-1.31837,-2.24061\n",
      "SGD(143/199): loss=0.2680373129774226, weights=-0.09569,-1.31099,-2.22909\n",
      "SGD(144/199): loss=0.2679434132594976, weights=-0.09963,-1.28346,-2.21145\n",
      "SGD(145/199): loss=0.2679335534190731, weights=-0.0771,-1.28921,-2.21131\n",
      "SGD(146/199): loss=0.26780261908142955, weights=-0.09132,-1.28769,-2.21949\n",
      "SGD(147/199): loss=0.2676899902448934, weights=-0.11319,-1.28439,-2.22872\n",
      "SGD(148/199): loss=0.26766636919928444, weights=-0.08539,-1.28187,-2.2199\n",
      "SGD(149/199): loss=0.26738369666566303, weights=-0.05006,-1.27211,-2.22042\n",
      "SGD(150/199): loss=0.26718510484805086, weights=-0.02019,-1.26695,-2.22565\n",
      "SGD(151/199): loss=0.26714451785201826, weights=-0.06266,-1.27542,-2.23604\n",
      "SGD(152/199): loss=0.2666230493595629, weights=-0.03897,-1.2596,-2.24669\n",
      "SGD(153/199): loss=0.2665359889427993, weights=-0.02006,-1.25469,-2.24725\n",
      "SGD(154/199): loss=0.2663353126755848, weights=-0.0188,-1.25997,-2.2618\n",
      "SGD(155/199): loss=0.26631538576328073, weights=-0.00889,-1.27477,-2.27706\n",
      "SGD(156/199): loss=0.26623358512116513, weights=-0.00538,-1.27126,-2.27849\n",
      "SGD(157/199): loss=0.265937736053872, weights=-0.03167,-1.24249,-2.26624\n",
      "SGD(158/199): loss=0.26585858816774427, weights=-0.02559,-1.25621,-2.28234\n",
      "SGD(159/199): loss=0.265741083084657, weights=-0.03628,-1.26779,-2.29905\n",
      "SGD(160/199): loss=0.2652338315516682, weights=-0.03386,-1.21509,-2.27846\n",
      "SGD(161/199): loss=0.2651532007606264, weights=-0.00408,-1.21954,-2.28812\n",
      "SGD(162/199): loss=0.26542871406964547, weights=0.03606,-1.19591,-2.26454\n",
      "SGD(163/199): loss=0.2653358120062465, weights=0.07149,-1.19796,-2.28407\n",
      "SGD(164/199): loss=0.2652460410618592, weights=0.09615,-1.17129,-2.28065\n",
      "SGD(165/199): loss=0.2651612611625458, weights=0.10938,-1.13528,-2.26738\n",
      "SGD(166/199): loss=0.26493066455736464, weights=0.09675,-1.14995,-2.28062\n",
      "SGD(167/199): loss=0.26478685963851595, weights=0.07315,-1.16378,-2.28535\n",
      "SGD(168/199): loss=0.2644209690518686, weights=0.02885,-1.15973,-2.2838\n",
      "SGD(169/199): loss=0.26434254115628886, weights=-0.00236,-1.12457,-2.25795\n",
      "SGD(170/199): loss=0.2640421726810083, weights=-0.01467,-1.12164,-2.26842\n",
      "SGD(171/199): loss=0.2640731346745481, weights=0.01543,-1.11453,-2.26663\n",
      "SGD(172/199): loss=0.26353144499330017, weights=-0.00223,-1.02307,-2.23978\n",
      "SGD(173/199): loss=0.26332096463389565, weights=-0.01334,-1.03732,-2.25375\n",
      "SGD(174/199): loss=0.26315741917533275, weights=-0.00582,-1.03989,-2.26237\n",
      "SGD(175/199): loss=0.2628450515098423, weights=-0.02876,-1.04499,-2.27689\n",
      "SGD(176/199): loss=0.26265657765097206, weights=-0.02925,-1.05411,-2.28981\n",
      "SGD(177/199): loss=0.2633634810425026, weights=-0.09735,-1.00166,-2.24507\n",
      "SGD(178/199): loss=0.263145406127844, weights=-0.09977,-1.0106,-2.258\n",
      "SGD(179/199): loss=0.26278600145634484, weights=-0.08544,-1.00049,-2.2648\n",
      "SGD(180/199): loss=0.26237607947692126, weights=-0.06159,-1.01159,-2.28238\n",
      "SGD(181/199): loss=0.262203010273175, weights=-0.05145,-1.01897,-2.29208\n",
      "SGD(182/199): loss=0.26233675067473494, weights=-0.03324,-0.99319,-2.27393\n",
      "SGD(183/199): loss=0.2621713442553382, weights=-0.04286,-1.00475,-2.28613\n",
      "SGD(184/199): loss=0.26254884835151854, weights=-0.07407,-0.96733,-2.25917\n",
      "SGD(185/199): loss=0.2622067552303956, weights=-0.06855,-0.98171,-2.27734\n",
      "SGD(186/199): loss=0.26307126614978377, weights=-0.14497,-0.98051,-2.26497\n",
      "SGD(187/199): loss=0.2626908238306746, weights=-0.12625,-0.99415,-2.27794\n",
      "SGD(188/199): loss=0.2620409566652971, weights=-0.10838,-0.99028,-2.29679\n",
      "SGD(189/199): loss=0.2616228569280284, weights=-0.09043,-0.99401,-2.31103\n",
      "SGD(190/199): loss=0.26141560681060555, weights=-0.08995,-1.0061,-2.32568\n",
      "SGD(191/199): loss=0.2610611014988929, weights=-0.05426,-1.0002,-2.33207\n",
      "SGD(192/199): loss=0.261238944879758, weights=-0.10138,-0.97223,-2.32012\n",
      "SGD(193/199): loss=0.2609783444041611, weights=-0.10819,-0.98276,-2.33807\n",
      "SGD(194/199): loss=0.26047491640509596, weights=-0.051,-0.96732,-2.34095\n",
      "SGD(195/199): loss=0.2602683466340833, weights=-0.03012,-0.94764,-2.33985\n",
      "SGD(196/199): loss=0.26003700741183505, weights=0.01152,-0.95043,-2.3544\n",
      "SGD(197/199): loss=0.25979940326268525, weights=0.01381,-0.96013,-2.36991\n",
      "SGD(198/199): loss=0.25962318148100666, weights=0.03512,-0.93992,-2.37246\n",
      "SGD(199/199): loss=0.2594150675609207, weights=0.03523,-0.95231,-2.38788\n"
     ]
    }
   ],
   "source": [
    "# demo for logistic_regression_SGD\n",
    "# define the parameters\n",
    "batch_size = 10\n",
    "max_iters = 200\n",
    "gamma = 0.3\n",
    "# weight initialization\n",
    "n = len(tx.T)\n",
    "initial_w = np.random.randn(n)\n",
    "# run SGD\n",
    "losses, ws = logistic_regression_SGD(y, tx, initial_w, max_iters, gamma, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required function(s)\n",
    "def reg_logistic_cost(y, tx, w, alpha):\n",
    "    \"\"\"cost for logistic regression with regularization\"\"\"\n",
    "    sig = sigmoid(tx.dot(w));\n",
    "    cost = (-y) * np.log(sig) - (1-y) * np.log(1-sig)\n",
    "    reg = np.dot(w,w) * alpha / (2 * len(y))\n",
    "    return np.mean(cost) + reg\n",
    "\n",
    "def reg_logistic_gradient(y, tx, w, alpha):\n",
    "    \"\"\"gradient for logistic regression with with regularization\"\"\"\n",
    "    err = sigmoid(tx.dot(w)) - y\n",
    "    grad = tx.T.dot(err) / len(err)\n",
    "    reg = w * alpha / len(err)\n",
    "    return grad - reg, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main function(s)\n",
    "def reg_logistic_regression(y, tx, alpha, initial_w, max_iters, gamma):\n",
    "    \"\"\"regularized logistic regression using GD\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient, loss\n",
    "        grad, _ = reg_logistic_gradient(y, tx, w, alpha)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # calculate loss\n",
    "        loss = reg_logistic_cost(y, tx, w, alpha)\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD({bi}/{ti}): loss={l}, weights={},{},{}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, *w.round(5)))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD(0/199): loss=0.49234066802359955, weights=0.35988,0.24664,-0.88955\n",
      "GD(1/199): loss=0.45993454561344516, weights=0.33566,0.18213,-0.96411\n",
      "GD(2/199): loss=0.4342775030547853, weights=0.31386,0.12555,-1.03081\n",
      "GD(3/199): loss=0.41358353419616983, weights=0.29417,0.07547,-1.09105\n",
      "GD(4/199): loss=0.3966071295335583, weights=0.2763,0.03075,-1.14595\n",
      "GD(5/199): loss=0.3824681134564932, weights=0.26003,-0.00948,-1.19638\n",
      "GD(6/199): loss=0.37053376243543135, weights=0.24514,-0.04591,-1.24304\n",
      "GD(7/199): loss=0.36034092216416064, weights=0.23146,-0.0791,-1.28645\n",
      "GD(8/199): loss=0.3515444868333099, weights=0.21885,-0.10949,-1.32707\n",
      "GD(9/199): loss=0.34388295743668174, weights=0.20718,-0.13744,-1.36525\n",
      "GD(10/199): loss=0.33715507710505427, weights=0.19635,-0.16324,-1.40129\n",
      "GD(11/199): loss=0.33120372098419687, weights=0.18628,-0.18716,-1.43542\n",
      "GD(12/199): loss=0.3259046030142275, weights=0.17687,-0.20939,-1.46785\n",
      "GD(13/199): loss=0.3211582307115727, weights=0.16807,-0.2301,-1.49876\n",
      "GD(14/199): loss=0.31688408499158605, weights=0.15983,-0.24946,-1.52829\n",
      "GD(15/199): loss=0.31301634820043184, weights=0.15208,-0.26758,-1.55658\n",
      "GD(16/199): loss=0.30950072570384396, weights=0.14479,-0.28458,-1.58373\n",
      "GD(17/199): loss=0.3062920509994808, weights=0.13792,-0.30055,-1.60984\n",
      "GD(18/199): loss=0.3033524598300248, weights=0.13143,-0.31559,-1.63499\n",
      "GD(19/199): loss=0.3006499827726174, weights=0.12529,-0.32976,-1.65926\n",
      "GD(20/199): loss=0.29815744927450355, weights=0.11948,-0.34313,-1.68271\n",
      "GD(21/199): loss=0.29585162606975685, weights=0.11398,-0.35576,-1.70541\n",
      "GD(22/199): loss=0.2937125338267069, weights=0.10875,-0.36771,-1.7274\n",
      "GD(23/199): loss=0.2917229006555125, weights=0.10378,-0.37902,-1.74873\n",
      "GD(24/199): loss=0.28986772167313035, weights=0.09906,-0.38972,-1.76944\n",
      "GD(25/199): loss=0.28813390146349216, weights=0.09456,-0.39988,-1.78958\n",
      "GD(26/199): loss=0.28650996185319294, weights=0.09027,-0.4095,-1.80917\n",
      "GD(27/199): loss=0.2849858015424966, weights=0.08618,-0.41864,-1.82826\n",
      "GD(28/199): loss=0.283552497200045, weights=0.08228,-0.42731,-1.84686\n",
      "GD(29/199): loss=0.2822021379358094, weights=0.07855,-0.43555,-1.86501\n",
      "GD(30/199): loss=0.2809276868146098, weights=0.07498,-0.44338,-1.88272\n",
      "GD(31/199): loss=0.2797228644077211, weights=0.07157,-0.45081,-1.90003\n",
      "GD(32/199): loss=0.2785820504078143, weights=0.0683,-0.45788,-1.91695\n",
      "GD(33/199): loss=0.277500200129226, weights=0.06517,-0.4646,-1.9335\n",
      "GD(34/199): loss=0.27647277333743875, weights=0.06216,-0.47099,-1.94969\n",
      "GD(35/199): loss=0.27549567334020997, weights=0.05928,-0.47706,-1.96556\n",
      "GD(36/199): loss=0.2745651946589563, weights=0.05652,-0.48283,-1.9811\n",
      "GD(37/199): loss=0.27367797790604015, weights=0.05386,-0.48831,-1.99634\n",
      "GD(38/199): loss=0.2728309707390853, weights=0.05131,-0.49352,-2.01128\n",
      "GD(39/199): loss=0.272021393960769, weights=0.04886,-0.49848,-2.02595\n",
      "GD(40/199): loss=0.27124671199194667, weights=0.0465,-0.50318,-2.04034\n",
      "GD(41/199): loss=0.2705046070753789, weights=0.04423,-0.50764,-2.05448\n",
      "GD(42/199): loss=0.2697929566728784, weights=0.04205,-0.51188,-2.06838\n",
      "GD(43/199): loss=0.26910981360516567, weights=0.03995,-0.5159,-2.08203\n",
      "GD(44/199): loss=0.2684533885548607, weights=0.03792,-0.5197,-2.09546\n",
      "GD(45/199): loss=0.2678220346118116, weights=0.03596,-0.52331,-2.10866\n",
      "GD(46/199): loss=0.26721423358869967, weights=0.03408,-0.52672,-2.12166\n",
      "GD(47/199): loss=0.2666285838754358, weights=0.03226,-0.52995,-2.13445\n",
      "GD(48/199): loss=0.26606378963476746, weights=0.03051,-0.533,-2.14704\n",
      "GD(49/199): loss=0.2655186511699333, weights=0.02882,-0.53588,-2.15945\n",
      "GD(50/199): loss=0.2649920563191161, weights=0.02718,-0.5386,-2.17166\n",
      "GD(51/199): loss=0.2644829727516232, weights=0.0256,-0.54115,-2.18371\n",
      "GD(52/199): loss=0.2639904410578022, weights=0.02408,-0.54355,-2.19558\n",
      "GD(53/199): loss=0.26351356853921215, weights=0.0226,-0.54581,-2.20728\n",
      "GD(54/199): loss=0.26305152361791173, weights=0.02118,-0.54792,-2.21882\n",
      "GD(55/199): loss=0.26260353079428633, weights=0.0198,-0.54989,-2.23021\n",
      "GD(56/199): loss=0.2621688660918566, weights=0.01847,-0.55173,-2.24144\n",
      "GD(57/199): loss=0.26174685293527217, weights=0.01718,-0.55344,-2.25253\n",
      "GD(58/199): loss=0.26133685841436316, weights=0.01593,-0.55503,-2.26348\n",
      "GD(59/199): loss=0.2609382898928832, weights=0.01472,-0.5565,-2.27428\n",
      "GD(60/199): loss=0.26055059192555236, weights=0.01355,-0.55785,-2.28496\n",
      "GD(61/199): loss=0.2601732434513283, weights=0.01241,-0.55909,-2.2955\n",
      "GD(62/199): loss=0.25980575523458055, weights=0.01132,-0.56022,-2.30592\n",
      "GD(63/199): loss=0.25944766752910725, weights=0.01025,-0.56124,-2.31621\n",
      "GD(64/199): loss=0.25909854794278125, weights=0.00922,-0.56216,-2.32638\n",
      "GD(65/199): loss=0.2587579894831006, weights=0.00822,-0.56299,-2.33644\n",
      "GD(66/199): loss=0.25842560876610066, weights=0.00725,-0.56371,-2.34638\n",
      "GD(67/199): loss=0.258101044372994, weights=0.00631,-0.56435,-2.35621\n",
      "GD(68/199): loss=0.2577839553405918, weights=0.00539,-0.56489,-2.36594\n",
      "GD(69/199): loss=0.2574740197730354, weights=0.00451,-0.56535,-2.37556\n",
      "GD(70/199): loss=0.25717093356367954, weights=0.00365,-0.56572,-2.38507\n",
      "GD(71/199): loss=0.25687440921711924, weights=0.00281,-0.56601,-2.39449\n",
      "GD(72/199): loss=0.2565841747623776, weights=0.002,-0.56622,-2.40381\n",
      "GD(73/199): loss=0.2562999727491776, weights=0.00122,-0.56635,-2.41303\n",
      "GD(74/199): loss=0.25602155932002535, weights=0.00045,-0.56641,-2.42216\n",
      "GD(75/199): loss=0.2557487033515493, weights=-0.00029,-0.56639,-2.4312\n",
      "GD(76/199): loss=0.2554811856591791, weights=-0.00101,-0.5663,-2.44016\n",
      "GD(77/199): loss=0.2552187982598121, weights=-0.00171,-0.56614,-2.44902\n",
      "GD(78/199): loss=0.2549613436876309, weights=-0.00239,-0.56592,-2.4578\n",
      "GD(79/199): loss=0.254708634358683, weights=-0.00305,-0.56563,-2.4665\n",
      "GD(80/199): loss=0.25446049198024634, weights=-0.0037,-0.56527,-2.47512\n",
      "GD(81/199): loss=0.25421674700136654, weights=-0.00432,-0.56486,-2.48365\n",
      "GD(82/199): loss=0.2539772381012811, weights=-0.00493,-0.56438,-2.49211\n",
      "GD(83/199): loss=0.25374181171274135, weights=-0.00553,-0.56384,-2.5005\n",
      "GD(84/199): loss=0.2535103215775058, weights=-0.0061,-0.56325,-2.50881\n",
      "GD(85/199): loss=0.2532826283315236, weights=-0.00666,-0.5626,-2.51705\n",
      "GD(86/199): loss=0.2530585991175348, weights=-0.00721,-0.5619,-2.52521\n",
      "GD(87/199): loss=0.252838107223018, weights=-0.00774,-0.56114,-2.53331\n",
      "GD(88/199): loss=0.2526210317415846, weights=-0.00825,-0.56033,-2.54134\n",
      "GD(89/199): loss=0.2524072572560854, weights=-0.00876,-0.55947,-2.5493\n",
      "GD(90/199): loss=0.2521966735418362, weights=-0.00925,-0.55857,-2.5572\n",
      "GD(91/199): loss=0.2519891752884999, weights=-0.00972,-0.55761,-2.56503\n",
      "GD(92/199): loss=0.2517846618392864, weights=-0.01019,-0.55661,-2.5728\n",
      "GD(93/199): loss=0.2515830369462362, weights=-0.01064,-0.55556,-2.58051\n",
      "GD(94/199): loss=0.25138420854045285, weights=-0.01108,-0.55447,-2.58816\n",
      "GD(95/199): loss=0.2511880885162416, weights=-0.01151,-0.55334,-2.59575\n",
      "GD(96/199): loss=0.25099459252819173, weights=-0.01193,-0.55216,-2.60328\n",
      "GD(97/199): loss=0.2508036398003155, weights=-0.01234,-0.55095,-2.61075\n",
      "GD(98/199): loss=0.2506151529464253, weights=-0.01273,-0.54969,-2.61816\n",
      "GD(99/199): loss=0.25042905780099267, weights=-0.01312,-0.54839,-2.62553\n",
      "GD(100/199): loss=0.2502452832597911, weights=-0.0135,-0.54706,-2.63283\n",
      "GD(101/199): loss=0.25006376112967543, weights=-0.01387,-0.54569,-2.64009\n",
      "GD(102/199): loss=0.2498844259869002, weights=-0.01422,-0.54429,-2.64729\n",
      "GD(103/199): loss=0.24970721504342253, weights=-0.01457,-0.54284,-2.65444\n",
      "GD(104/199): loss=0.24953206802067618, weights=-0.01491,-0.54137,-2.66154\n",
      "GD(105/199): loss=0.24935892703033935, weights=-0.01525,-0.53986,-2.66859\n",
      "GD(106/199): loss=0.24918773646165548, weights=-0.01557,-0.53832,-2.67559\n",
      "GD(107/199): loss=0.2490184428748948, weights=-0.01589,-0.53674,-2.68255\n",
      "GD(108/199): loss=0.2488509949005755, weights=-0.0162,-0.53514,-2.68945\n",
      "GD(109/199): loss=0.24868534314408933, weights=-0.0165,-0.5335,-2.69632\n",
      "GD(110/199): loss=0.24852144009540142, weights=-0.01679,-0.53184,-2.70313\n",
      "GD(111/199): loss=0.24835924004351567, weights=-0.01708,-0.53014,-2.7099\n",
      "GD(112/199): loss=0.24819869899542035, weights=-0.01736,-0.52842,-2.71663\n",
      "GD(113/199): loss=0.24803977459924512, weights=-0.01763,-0.52667,-2.72331\n",
      "GD(114/199): loss=0.24788242607138167, weights=-0.0179,-0.52489,-2.72995\n",
      "GD(115/199): loss=0.24772661412733343, weights=-0.01816,-0.52309,-2.73655\n",
      "GD(116/199): loss=0.2475723009160788, weights=-0.01841,-0.52126,-2.74311\n",
      "GD(117/199): loss=0.24741944995774307, weights=-0.01866,-0.51941,-2.74963\n",
      "GD(118/199): loss=0.24726802608439047, weights=-0.0189,-0.51753,-2.75611\n",
      "GD(119/199): loss=0.24711799538375784, weights=-0.01914,-0.51563,-2.76254\n",
      "GD(120/199): loss=0.24696932514576417, weights=-0.01937,-0.5137,-2.76894\n",
      "GD(121/199): loss=0.2468219838116394, weights=-0.0196,-0.51176,-2.7753\n",
      "GD(122/199): loss=0.24667594092552808, weights=-0.01982,-0.50979,-2.78163\n",
      "GD(123/199): loss=0.24653116708842945, weights=-0.02003,-0.50779,-2.78791\n",
      "GD(124/199): loss=0.246387633914347, weights=-0.02024,-0.50578,-2.79416\n",
      "GD(125/199): loss=0.2462453139885263, weights=-0.02045,-0.50375,-2.80038\n",
      "GD(126/199): loss=0.24610418082766883, weights=-0.02065,-0.50169,-2.80656\n",
      "GD(127/199): loss=0.24596420884201478, weights=-0.02085,-0.49962,-2.8127\n",
      "GD(128/199): loss=0.24582537329919607, weights=-0.02104,-0.49753,-2.81881\n",
      "GD(129/199): loss=0.24568765028976514, weights=-0.02122,-0.49542,-2.82489\n",
      "GD(130/199): loss=0.24555101669431206, weights=-0.02141,-0.49329,-2.83093\n",
      "GD(131/199): loss=0.24541545015208574, weights=-0.02159,-0.49114,-2.83694\n",
      "GD(132/199): loss=0.245280929031043, weights=-0.02176,-0.48898,-2.84292\n",
      "GD(133/199): loss=0.2451474323992504, weights=-0.02193,-0.48679,-2.84886\n",
      "GD(134/199): loss=0.24501493999756965, weights=-0.0221,-0.4846,-2.85478\n",
      "GD(135/199): loss=0.24488343221356318, weights=-0.02226,-0.48238,-2.86066\n",
      "GD(136/199): loss=0.24475289005655523, weights=-0.02242,-0.48015,-2.86651\n",
      "GD(137/199): loss=0.24462329513379297, weights=-0.02258,-0.4779,-2.87233\n",
      "GD(138/199): loss=0.2444946296276519, weights=-0.02273,-0.47564,-2.87812\n",
      "GD(139/199): loss=0.24436687627383327, weights=-0.02288,-0.47337,-2.88389\n",
      "GD(140/199): loss=0.24424001834050588, weights=-0.02302,-0.47108,-2.88962\n",
      "GD(141/199): loss=0.24411403960834432, weights=-0.02317,-0.46878,-2.89532\n",
      "GD(142/199): loss=0.24398892435142228, weights=-0.02331,-0.46646,-2.901\n",
      "GD(143/199): loss=0.24386465731891785, weights=-0.02344,-0.46413,-2.90665\n",
      "GD(144/199): loss=0.24374122371759205, weights=-0.02358,-0.46178,-2.91227\n",
      "GD(145/199): loss=0.24361860919500472, weights=-0.02371,-0.45943,-2.91786\n",
      "GD(146/199): loss=0.24349679982343178, weights=-0.02383,-0.45706,-2.92343\n",
      "GD(147/199): loss=0.24337578208445135, weights=-0.02396,-0.45468,-2.92897\n",
      "GD(148/199): loss=0.24325554285416637, weights=-0.02408,-0.45229,-2.93448\n",
      "GD(149/199): loss=0.24313606938903604, weights=-0.0242,-0.44988,-2.93997\n",
      "GD(150/199): loss=0.24301734931228577, weights=-0.02432,-0.44747,-2.94543\n",
      "GD(151/199): loss=0.24289937060087008, weights=-0.02443,-0.44504,-2.95087\n",
      "GD(152/199): loss=0.2427821215729631, weights=-0.02454,-0.44261,-2.95628\n",
      "GD(153/199): loss=0.24266559087595257, weights=-0.02465,-0.44016,-2.96167\n",
      "GD(154/199): loss=0.24254976747491433, weights=-0.02476,-0.43771,-2.96703\n",
      "GD(155/199): loss=0.24243464064154535, weights=-0.02486,-0.43524,-2.97237\n",
      "GD(156/199): loss=0.24232019994353637, weights=-0.02496,-0.43277,-2.97769\n",
      "GD(157/199): loss=0.24220643523436233, weights=-0.02506,-0.43028,-2.98298\n",
      "GD(158/199): loss=0.24209333664347382, weights=-0.02516,-0.42779,-2.98825\n",
      "GD(159/199): loss=0.24198089456687114, weights=-0.02526,-0.42529,-2.9935\n",
      "GD(160/199): loss=0.24186909965804404, weights=-0.02535,-0.42278,-2.99872\n",
      "GD(161/199): loss=0.24175794281926236, weights=-0.02544,-0.42026,-3.00392\n",
      "GD(162/199): loss=0.24164741519320046, weights=-0.02553,-0.41773,-3.0091\n",
      "GD(163/199): loss=0.24153750815488303, weights=-0.02562,-0.4152,-3.01426\n",
      "GD(164/199): loss=0.2414282133039371, weights=-0.02571,-0.41265,-3.01939\n",
      "GD(165/199): loss=0.2413195224571377, weights=-0.02579,-0.41011,-3.02451\n",
      "GD(166/199): loss=0.24121142764123507, weights=-0.02587,-0.40755,-3.0296\n",
      "GD(167/199): loss=0.24110392108605086, weights=-0.02595,-0.40499,-3.03467\n",
      "GD(168/199): loss=0.2409969952178323, weights=-0.02603,-0.40242,-3.03972\n",
      "GD(169/199): loss=0.24089064265285373, weights=-0.02611,-0.39984,-3.04475\n",
      "GD(170/199): loss=0.24078485619125517, weights=-0.02618,-0.39726,-3.04976\n",
      "GD(171/199): loss=0.24067962881110774, weights=-0.02626,-0.39467,-3.05476\n",
      "GD(172/199): loss=0.240574953662697, weights=-0.02633,-0.39207,-3.05973\n",
      "GD(173/199): loss=0.24047082406301523, weights=-0.0264,-0.38947,-3.06468\n",
      "GD(174/199): loss=0.24036723349045397, weights=-0.02647,-0.38686,-3.06961\n",
      "GD(175/199): loss=0.24026417557968888, weights=-0.02654,-0.38425,-3.07452\n",
      "GD(176/199): loss=0.24016164411674876, weights=-0.0266,-0.38163,-3.07942\n",
      "GD(177/199): loss=0.24005963303426214, weights=-0.02667,-0.37901,-3.08429\n",
      "GD(178/199): loss=0.23995813640687325, weights=-0.02673,-0.37639,-3.08915\n",
      "GD(179/199): loss=0.23985714844682154, weights=-0.02679,-0.37375,-3.09399\n",
      "GD(180/199): loss=0.2397566634996781, weights=-0.02685,-0.37112,-3.09881\n",
      "GD(181/199): loss=0.2396566760402318, weights=-0.02691,-0.36848,-3.10361\n",
      "GD(182/199): loss=0.23955718066852122, weights=-0.02697,-0.36583,-3.1084\n",
      "GD(183/199): loss=0.23945817210600467, weights=-0.02703,-0.36318,-3.11316\n",
      "GD(184/199): loss=0.2393596451918642, weights=-0.02708,-0.36053,-3.11791\n",
      "GD(185/199): loss=0.23926159487943766, weights=-0.02714,-0.35787,-3.12265\n",
      "GD(186/199): loss=0.23916401623277472, weights=-0.02719,-0.35521,-3.12736\n",
      "GD(187/199): loss=0.2390669044233111, weights=-0.02724,-0.35255,-3.13206\n",
      "GD(188/199): loss=0.23897025472665717, weights=-0.02729,-0.34988,-3.13674\n",
      "GD(189/199): loss=0.23887406251949644, weights=-0.02734,-0.34721,-3.14141\n",
      "GD(190/199): loss=0.23877832327658957, weights=-0.02739,-0.34454,-3.14606\n",
      "GD(191/199): loss=0.23868303256787987, weights=-0.02744,-0.34186,-3.15069\n",
      "GD(192/199): loss=0.23858818605569715, weights=-0.02748,-0.33918,-3.15531\n",
      "GD(193/199): loss=0.23849377949205539, weights=-0.02753,-0.3365,-3.15991\n",
      "GD(194/199): loss=0.23839980871604072, weights=-0.02757,-0.33381,-3.16449\n",
      "GD(195/199): loss=0.23830626965128765, weights=-0.02762,-0.33112,-3.16906\n",
      "GD(196/199): loss=0.2382131583035385, weights=-0.02766,-0.32844,-3.17361\n",
      "GD(197/199): loss=0.23812047075828405, weights=-0.0277,-0.32574,-3.17815\n",
      "GD(198/199): loss=0.2380282031784825, weights=-0.02774,-0.32305,-3.18268\n",
      "GD(199/199): loss=0.2379363518023531, weights=-0.02778,-0.32035,-3.18718\n"
     ]
    }
   ],
   "source": [
    "# demo for reg_logistic_regression\n",
    "# define the parameters\n",
    "alpha = 0.01\n",
    "max_iters = 200\n",
    "gamma = 0.3\n",
    "# weight initialization\n",
    "n = num_features = len(tx.T)\n",
    "initial_w = np.random.randn(n)\n",
    "# run GD\n",
    "losses, ws = reg_logistic_regression(y, tx, alpha, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression_SGD(y, tx, alpha, initial_w, max_iters, gamma, batch_size=1):\n",
    "    \"\"\"regularized logistic regression using SGD\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = reg_logistic_gradient(y_batch, tx_batch, w, alpha)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = reg_logistic_cost(y, tx, w, alpha)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, weights={},{},{}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, *w.round(5)))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/199): loss=1.118359449652398, weights=1.24238,-0.88944,1.307\n",
      "SGD(1/199): loss=0.9051363941932274, weights=1.00462,-1.10076,1.13659\n",
      "SGD(2/199): loss=0.8865239881879458, weights=1.08969,-1.15623,1.08746\n",
      "SGD(3/199): loss=0.9069208223645194, weights=1.16358,-1.16015,1.09599\n",
      "SGD(4/199): loss=0.8948778537706535, weights=1.25297,-1.20182,1.03924\n",
      "SGD(5/199): loss=0.8880402365854403, weights=1.36195,-1.22274,0.95781\n",
      "SGD(6/199): loss=0.7117307867328309, weights=1.16437,-1.47734,0.77589\n",
      "SGD(7/199): loss=0.6136816787666682, weights=1.02414,-1.66835,0.65415\n",
      "SGD(8/199): loss=0.6095799338735564, weights=1.03406,-1.68979,0.64281\n",
      "SGD(9/199): loss=0.6041002142658052, weights=1.079,-1.72895,0.60626\n",
      "SGD(10/199): loss=0.5327912993906979, weights=0.97306,-1.89268,0.4508\n",
      "SGD(11/199): loss=0.49404651224313195, weights=0.89974,-2.00566,0.34831\n",
      "SGD(12/199): loss=0.4668183853334942, weights=0.78847,-2.09794,0.30735\n",
      "SGD(13/199): loss=0.46432548811244984, weights=0.80987,-2.12539,0.28178\n",
      "SGD(14/199): loss=0.4457350274770739, weights=0.76382,-2.19772,0.20775\n",
      "SGD(15/199): loss=0.4355181989602702, weights=0.72915,-2.25359,0.17216\n",
      "SGD(16/199): loss=0.4197084708066366, weights=0.67167,-2.32224,0.09804\n",
      "SGD(17/199): loss=0.3964115354570955, weights=0.54667,-2.38814,-0.02715\n",
      "SGD(18/199): loss=0.3836219830589825, weights=0.4624,-2.44688,-0.10952\n",
      "SGD(19/199): loss=0.3832942793041698, weights=0.47695,-2.4677,-0.12366\n",
      "SGD(20/199): loss=0.3720960142629946, weights=0.28687,-2.46666,-0.15083\n",
      "SGD(21/199): loss=0.3713914676643483, weights=0.3066,-2.49131,-0.17122\n",
      "SGD(22/199): loss=0.3698726451881358, weights=0.29697,-2.51297,-0.18829\n",
      "SGD(23/199): loss=0.3646368470102045, weights=0.08309,-2.46714,-0.18571\n",
      "SGD(24/199): loss=0.3572694661677653, weights=-0.09976,-2.43943,-0.29208\n",
      "SGD(25/199): loss=0.3566049304630353, weights=-0.17476,-2.47417,-0.32076\n",
      "SGD(26/199): loss=0.36367092471175394, weights=-0.33985,-2.46265,-0.2782\n",
      "SGD(27/199): loss=0.3615756149626657, weights=-0.40454,-2.48685,-0.35263\n",
      "SGD(28/199): loss=0.35835934914487305, weights=-0.36752,-2.52562,-0.3853\n",
      "SGD(29/199): loss=0.3537350500850064, weights=-0.29997,-2.56887,-0.43458\n",
      "SGD(30/199): loss=0.35373313438937765, weights=-0.30142,-2.57763,-0.43718\n",
      "SGD(31/199): loss=0.3533711804207541, weights=-0.32296,-2.59996,-0.46118\n",
      "SGD(32/199): loss=0.353408990450004, weights=-0.32435,-2.60864,-0.46335\n",
      "SGD(33/199): loss=0.35225958482630554, weights=-0.30749,-2.63438,-0.48265\n",
      "SGD(34/199): loss=0.3501729379670453, weights=-0.26928,-2.66932,-0.51529\n",
      "SGD(35/199): loss=0.350066981338135, weights=-0.26762,-2.68124,-0.52078\n",
      "SGD(36/199): loss=0.3500884177396853, weights=-0.27059,-2.69237,-0.5258\n",
      "SGD(37/199): loss=0.3496214741397837, weights=-0.26468,-2.70831,-0.53883\n",
      "SGD(38/199): loss=0.35169209272665336, weights=-0.3709,-2.72488,-0.56611\n",
      "SGD(39/199): loss=0.34481065357385954, weights=-0.22269,-2.7382,-0.64326\n",
      "SGD(40/199): loss=0.34484106361344974, weights=-0.2434,-2.75974,-0.66368\n",
      "SGD(41/199): loss=0.34507404871956393, weights=-0.26426,-2.78173,-0.6811\n",
      "SGD(42/199): loss=0.34511422322216356, weights=-0.26342,-2.79261,-0.68593\n",
      "SGD(43/199): loss=0.35079368025111207, weights=-0.47053,-2.72688,-0.67303\n",
      "SGD(44/199): loss=0.35090573314824197, weights=-0.47167,-2.73567,-0.6755\n",
      "SGD(45/199): loss=0.34822087340846974, weights=-0.43618,-2.76583,-0.72172\n",
      "SGD(46/199): loss=0.3480251555698775, weights=-0.43052,-2.78285,-0.73076\n",
      "SGD(47/199): loss=0.3480765449936666, weights=-0.43056,-2.79322,-0.7354\n",
      "SGD(48/199): loss=0.35052750354645273, weights=-0.48308,-2.82172,-0.74125\n",
      "SGD(49/199): loss=0.3508371906310389, weights=-0.48932,-2.83521,-0.74786\n",
      "SGD(50/199): loss=0.35102360145340855, weights=-0.49131,-2.84462,-0.7508\n",
      "SGD(51/199): loss=0.3512578854002537, weights=-0.49439,-2.85541,-0.75461\n",
      "SGD(52/199): loss=0.3536885177922394, weights=-0.53575,-2.88374,-0.75506\n",
      "SGD(53/199): loss=0.3527812074549583, weights=-0.51986,-2.90757,-0.77624\n",
      "SGD(54/199): loss=0.3438418831231919, weights=-0.25592,-2.79409,-0.71613\n",
      "SGD(55/199): loss=0.34351760139754345, weights=-0.24108,-2.8167,-0.73159\n",
      "SGD(56/199): loss=0.3434653201942392, weights=-0.23606,-2.83186,-0.74074\n",
      "SGD(57/199): loss=0.34319443555882106, weights=-0.22161,-2.8537,-0.75682\n",
      "SGD(58/199): loss=0.34333823241465394, weights=-0.22459,-2.86509,-0.76257\n",
      "SGD(59/199): loss=0.3473632668470571, weights=0.07065,-2.52991,-0.48172\n",
      "SGD(60/199): loss=0.3442882203084616, weights=-0.04556,-2.55108,-0.53898\n",
      "SGD(61/199): loss=0.3442561114207867, weights=-0.04359,-2.56236,-0.54303\n",
      "SGD(62/199): loss=0.34426212765008746, weights=-0.04441,-2.57143,-0.54581\n",
      "SGD(63/199): loss=0.3413072489078904, weights=-0.17043,-2.57477,-0.63261\n",
      "SGD(64/199): loss=0.3422945956652989, weights=-0.24452,-2.60546,-0.65198\n",
      "SGD(65/199): loss=0.34278577296372426, weights=-0.28537,-2.63322,-0.67416\n",
      "SGD(66/199): loss=0.3402888799781077, weights=-0.06477,-2.58725,-0.63983\n",
      "SGD(67/199): loss=0.3391979642270906, weights=0.06366,-2.60265,-0.68156\n",
      "SGD(68/199): loss=0.33887589602094004, weights=0.00373,-2.64008,-0.69819\n",
      "SGD(69/199): loss=0.3389425116419871, weights=0.00111,-2.65211,-0.70255\n",
      "SGD(70/199): loss=0.33757810834436375, weights=0.28541,-2.34329,-0.7282\n",
      "SGD(71/199): loss=0.33800470371758967, weights=-0.00995,-1.87678,-0.59676\n",
      "SGD(72/199): loss=0.33687006358071225, weights=-0.02043,-1.89671,-0.6116\n",
      "SGD(73/199): loss=0.33660870007353993, weights=-0.02105,-1.90373,-0.61494\n",
      "SGD(74/199): loss=0.33532942478368677, weights=-0.04289,-1.93308,-0.63309\n",
      "SGD(75/199): loss=0.33449793081070117, weights=-0.09529,-1.97506,-0.64943\n",
      "SGD(76/199): loss=0.3341102165036692, weights=0.08899,-1.94203,-0.66357\n",
      "SGD(77/199): loss=0.3342731425819281, weights=0.19192,-1.96825,-0.69321\n",
      "SGD(78/199): loss=0.33256953437031983, weights=0.17449,-1.99398,-0.71542\n",
      "SGD(79/199): loss=0.3274690518617542, weights=0.04647,-2.01003,-0.77262\n",
      "SGD(80/199): loss=0.32597936334879346, weights=0.17496,-2.00731,-0.83828\n",
      "SGD(81/199): loss=0.3263068449940004, weights=0.28538,-2.01138,-0.89438\n",
      "SGD(82/199): loss=0.32715267772019135, weights=0.32622,-2.0423,-0.91205\n",
      "SGD(83/199): loss=0.32712606806932865, weights=0.33274,-2.05587,-0.92025\n",
      "SGD(84/199): loss=0.3312651883835303, weights=0.44728,-2.06282,-0.94143\n",
      "SGD(85/199): loss=0.33126125411710405, weights=0.45367,-2.07499,-0.95056\n",
      "SGD(86/199): loss=0.3298336756023342, weights=0.43931,-2.09853,-0.97099\n",
      "SGD(87/199): loss=0.3298251288123079, weights=0.44095,-2.10549,-0.97456\n",
      "SGD(88/199): loss=0.3297967370065426, weights=0.45012,-2.11856,-0.98848\n",
      "SGD(89/199): loss=0.32927236515868713, weights=0.44645,-2.1317,-0.99986\n",
      "SGD(90/199): loss=0.32889874467375774, weights=0.44174,-2.14801,-1.00794\n",
      "SGD(91/199): loss=0.3288461541255931, weights=0.44228,-2.15616,-1.01227\n",
      "SGD(92/199): loss=0.32907104211409055, weights=0.45211,-2.17123,-1.02297\n",
      "SGD(93/199): loss=0.3328459903046232, weights=0.55779,-2.16713,-1.06658\n",
      "SGD(94/199): loss=0.33291647013252834, weights=0.56008,-2.17484,-1.07059\n",
      "SGD(95/199): loss=0.3330391554146644, weights=0.56436,-2.18478,-1.077\n",
      "SGD(96/199): loss=0.33373207552058853, weights=0.59011,-2.2006,-1.10333\n",
      "SGD(97/199): loss=0.3331116434619473, weights=0.58228,-2.22058,-1.11476\n",
      "SGD(98/199): loss=0.33265014229556167, weights=0.5784,-2.23453,-1.12649\n",
      "SGD(99/199): loss=0.33265460234088945, weights=0.5793,-2.24291,-1.13133\n",
      "SGD(100/199): loss=0.331682261338641, weights=0.56264,-2.27162,-1.14492\n",
      "SGD(101/199): loss=0.32987046036844336, weights=0.54253,-2.29418,-1.17632\n",
      "SGD(102/199): loss=0.32398128303381996, weights=0.38743,-2.33951,-1.16517\n",
      "SGD(103/199): loss=0.3240661506525117, weights=0.3887,-2.34675,-1.16889\n",
      "SGD(104/199): loss=0.324159088962478, weights=0.39011,-2.35429,-1.17281\n",
      "SGD(105/199): loss=0.3269464262551177, weights=0.48678,-2.35585,-1.21759\n",
      "SGD(106/199): loss=0.3344442945632601, weights=0.64464,-2.32063,-1.22958\n",
      "SGD(107/199): loss=0.3345708160303071, weights=0.64654,-2.32768,-1.23336\n",
      "SGD(108/199): loss=0.33462028808719313, weights=0.64749,-2.33647,-1.23876\n",
      "SGD(109/199): loss=0.3438961254506557, weights=0.78816,-2.31664,-1.23745\n",
      "SGD(110/199): loss=0.3436785663577476, weights=0.78678,-2.32903,-1.2466\n",
      "SGD(111/199): loss=0.34169047751587367, weights=0.76184,-2.36212,-1.2692\n",
      "SGD(112/199): loss=0.3421156082551699, weights=0.76861,-2.37338,-1.27732\n",
      "SGD(113/199): loss=0.34246834434080903, weights=0.7742,-2.38371,-1.2848\n",
      "SGD(114/199): loss=0.34239154010301265, weights=0.77409,-2.39414,-1.29317\n",
      "SGD(115/199): loss=0.3426384768104496, weights=0.77746,-2.40277,-1.29832\n",
      "SGD(116/199): loss=0.3360363399215842, weights=0.69979,-2.43488,-1.36637\n",
      "SGD(117/199): loss=0.3362510782560169, weights=0.70238,-2.44295,-1.37118\n",
      "SGD(118/199): loss=0.3367923347834671, weights=0.70961,-2.45543,-1.37861\n",
      "SGD(119/199): loss=0.3385385733302516, weights=0.73734,-2.47324,-1.39503\n",
      "SGD(120/199): loss=0.322246669666611, weights=0.50474,-2.39937,-1.45172\n",
      "SGD(121/199): loss=0.32233957466242724, weights=0.50408,-2.41014,-1.45829\n",
      "SGD(122/199): loss=0.3224965205427681, weights=0.50561,-2.41742,-1.4627\n",
      "SGD(123/199): loss=0.3232638985154832, weights=0.52117,-2.43274,-1.47784\n",
      "SGD(124/199): loss=0.3236533971712689, weights=0.52794,-2.44413,-1.48791\n",
      "SGD(125/199): loss=0.32389052606317165, weights=0.53064,-2.45301,-1.49365\n",
      "SGD(126/199): loss=0.3273975735375986, weights=0.60726,-2.46405,-1.52059\n",
      "SGD(127/199): loss=0.31796861825641826, weights=0.41023,-2.45749,-1.53879\n",
      "SGD(128/199): loss=0.3105257003151289, weights=0.17071,-2.39676,-1.49699\n",
      "SGD(129/199): loss=0.3113731811597006, weights=0.21174,-2.41823,-1.52425\n",
      "SGD(130/199): loss=0.31071204733856383, weights=0.09216,-2.4594,-1.52346\n",
      "SGD(131/199): loss=0.30988793182812646, weights=0.01747,-2.47906,-1.56683\n",
      "SGD(132/199): loss=0.3096741970017154, weights=-0.07697,-2.49681,-1.60313\n",
      "SGD(133/199): loss=0.3098682458024489, weights=-0.0815,-2.50804,-1.61326\n",
      "SGD(134/199): loss=0.3102234138607956, weights=-0.09378,-2.5249,-1.62667\n",
      "SGD(135/199): loss=0.31040067092630513, weights=-0.09367,-2.53315,-1.63212\n",
      "SGD(136/199): loss=0.3110158902719131, weights=-0.12242,-2.55665,-1.65006\n",
      "SGD(137/199): loss=0.31161605246634944, weights=-0.14304,-2.57772,-1.66497\n",
      "SGD(138/199): loss=0.3118299406401179, weights=-0.13714,-2.59077,-1.67686\n",
      "SGD(139/199): loss=0.31205179258140475, weights=-0.14049,-2.60082,-1.68623\n",
      "SGD(140/199): loss=0.31257134717071733, weights=-0.15513,-2.6187,-1.69985\n",
      "SGD(141/199): loss=0.31290894358299653, weights=-0.14491,-2.63589,-1.71229\n",
      "SGD(142/199): loss=0.3133456903851604, weights=-0.1565,-2.65141,-1.72596\n",
      "SGD(143/199): loss=0.31367720305046287, weights=-0.20461,-2.66339,-1.76665\n",
      "SGD(144/199): loss=0.31389630868737894, weights=-0.19761,-2.67675,-1.78044\n",
      "SGD(145/199): loss=0.3130040932108997, weights=-0.0035,-2.66674,-1.7673\n",
      "SGD(146/199): loss=0.3139207811884726, weights=0.10566,-2.68041,-1.79875\n",
      "SGD(147/199): loss=0.31518654559525444, weights=0.20367,-2.68806,-1.83853\n",
      "SGD(148/199): loss=0.31554148435693, weights=0.20649,-2.69868,-1.84597\n",
      "SGD(149/199): loss=0.3033740166832969, weights=-0.0917,-2.28731,-1.60379\n",
      "SGD(150/199): loss=0.3036139890891554, weights=-0.12173,-2.30745,-1.62889\n",
      "SGD(151/199): loss=0.3036739377214004, weights=-0.09908,-2.32817,-1.65111\n",
      "SGD(152/199): loss=0.303804053167948, weights=-0.09908,-2.33582,-1.65639\n",
      "SGD(153/199): loss=0.3040674395084198, weights=-0.13194,-2.3548,-1.68392\n",
      "SGD(154/199): loss=0.30420091580945563, weights=-0.13248,-2.36212,-1.68924\n",
      "SGD(155/199): loss=0.3038648556603414, weights=-0.00032,-2.37371,-1.71674\n",
      "SGD(156/199): loss=0.30117699577956225, weights=-0.28445,-2.11768,-1.60831\n",
      "SGD(157/199): loss=0.29999225751827446, weights=-0.19101,-2.15593,-1.63354\n",
      "SGD(158/199): loss=0.300136001260688, weights=-0.19418,-2.16595,-1.64101\n",
      "SGD(159/199): loss=0.30009982314125494, weights=-0.18615,-2.17991,-1.65559\n",
      "SGD(160/199): loss=0.3007091281444824, weights=-0.2206,-2.20219,-1.67821\n",
      "SGD(161/199): loss=0.3007984328720563, weights=-0.21997,-2.21073,-1.68506\n",
      "SGD(162/199): loss=0.3007938112175498, weights=-0.21369,-2.22333,-1.69861\n",
      "SGD(163/199): loss=0.3008843266141242, weights=-0.21314,-2.2316,-1.70565\n",
      "SGD(164/199): loss=0.30093168223715244, weights=-0.20642,-2.24507,-1.71871\n",
      "SGD(165/199): loss=0.29844469472363194, weights=0.00552,-2.19229,-1.71727\n",
      "SGD(166/199): loss=0.2990946685974993, weights=0.0949,-2.21205,-1.74993\n",
      "SGD(167/199): loss=0.29981630975430157, weights=0.13937,-2.23371,-1.77814\n",
      "SGD(168/199): loss=0.30045828993228985, weights=0.16792,-2.25302,-1.80131\n",
      "SGD(169/199): loss=0.30038213911014744, weights=0.14792,-2.27233,-1.82267\n",
      "SGD(170/199): loss=0.3004870699177213, weights=0.13753,-2.28804,-1.83745\n",
      "SGD(171/199): loss=0.30095492879274893, weights=0.15274,-2.30387,-1.8545\n",
      "SGD(172/199): loss=0.30110936688186213, weights=0.15338,-2.31114,-1.86035\n",
      "SGD(173/199): loss=0.29475698668457495, weights=-0.10504,-2.14309,-1.84863\n",
      "SGD(174/199): loss=0.2958174200752146, weights=-0.01118,-2.18677,-1.85617\n",
      "SGD(175/199): loss=0.2953023155045456, weights=0.05411,-2.18769,-1.91379\n",
      "SGD(176/199): loss=0.2955522709728125, weights=0.05904,-2.19932,-1.9239\n",
      "SGD(177/199): loss=0.29571875161315925, weights=0.06136,-2.20808,-1.93262\n",
      "SGD(178/199): loss=0.29604154278063377, weights=0.07155,-2.22203,-1.94716\n",
      "SGD(179/199): loss=0.2957236642691176, weights=-0.0283,-2.24142,-1.97773\n",
      "SGD(180/199): loss=0.29594244886154286, weights=-0.03082,-2.25151,-1.98573\n",
      "SGD(181/199): loss=0.29614874444701855, weights=-0.02572,-2.2623,-1.99776\n",
      "SGD(182/199): loss=0.2966819136886707, weights=-0.04198,-2.28371,-2.01019\n",
      "SGD(183/199): loss=0.29513771340183076, weights=-0.2383,-2.20717,-2.04473\n",
      "SGD(184/199): loss=0.294861301791338, weights=-0.1769,-2.23379,-2.07731\n",
      "SGD(185/199): loss=0.2951196354874692, weights=-0.15648,-2.25387,-2.09705\n",
      "SGD(186/199): loss=0.2957746448618992, weights=-0.19428,-2.27159,-2.12351\n",
      "SGD(187/199): loss=0.29601479361743466, weights=-0.19691,-2.28059,-2.13217\n",
      "SGD(188/199): loss=0.2962850944903282, weights=-0.18915,-2.29488,-2.14529\n",
      "SGD(189/199): loss=0.2958979360783382, weights=-0.02692,-2.30453,-2.15014\n",
      "SGD(190/199): loss=0.29618205426173355, weights=-0.03222,-2.31601,-2.16143\n",
      "SGD(191/199): loss=0.2964369601116601, weights=-0.10846,-2.32965,-2.1976\n",
      "SGD(192/199): loss=0.2967578967202259, weights=-0.10057,-2.34287,-2.21134\n",
      "SGD(193/199): loss=0.2970094952894561, weights=-0.10196,-2.35157,-2.21892\n",
      "SGD(194/199): loss=0.29730876489461333, weights=-0.10596,-2.36188,-2.22927\n",
      "SGD(195/199): loss=0.2975519982695404, weights=-0.10355,-2.37113,-2.23954\n",
      "SGD(196/199): loss=0.2984092365528769, weights=-0.07135,-2.40006,-2.25525\n",
      "SGD(197/199): loss=0.2991226829912659, weights=-0.05205,-2.42248,-2.26951\n",
      "SGD(198/199): loss=0.2993569633385096, weights=-0.05245,-2.43012,-2.27667\n",
      "SGD(199/199): loss=0.29969496066112267, weights=-0.04991,-2.44077,-2.28556\n"
     ]
    }
   ],
   "source": [
    "# demo for reg_logistic_regression_SGD\n",
    "# define the parameters\n",
    "batch_size = 10\n",
    "alpha = 0.01\n",
    "max_iters = 200\n",
    "gamma = 0.3\n",
    "# weight initialization\n",
    "n = num_features = len(tx.T)\n",
    "initial_w = np.random.randn(n)\n",
    "# run GD\n",
    "losses, ws = reg_logistic_regression_SGD(y, tx, alpha, initial_w, max_iters, gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
